https://chatgpt.com/c/698ee3d5-a970-83a5-8f03-52dd6784bd79

Below is a structured analysis based on:

* the **AI Futures Model: Dec 2025 Update** (Dec 31, 2025) ([AI Futures][1])
* the authors’ **“Grading AI 2027’s 2025 Predictions”** post (Feb 12, 2026) ([AI Futures][2])
* their associated **grading spreadsheet** ([Google Docs][3])
* the **AI 2027 timelines forecast** research note (Apr 2025, with later updates noted on-page) ([AI 2027][4])
* **Gary Marcus’s critique** (May 22, 2025) ([Gary Marcus][5])
* the **AI Futures Model forecast snapshot** I could access for Daniel (Jan 26, 2026) ([AI Futures Model][6])

I’m treating the “grading” post + spreadsheet as the most direct “reviewing predictions so far,” and the Dec 2025 update as the model update that motivated later re-evaluations.

---

## 1) The authors’ criteria and how to operationalize them for independent review

### 1.1 Their core “pace of progress” concept

In the Feb 12, 2026 review, they define a **pace multiplier** for quantitative predictions:

* **1×** = reality is moving at the pace AI 2027 predicted
* **2×** = twice as fast
* **0.5×** = half as fast ([AI Futures][2])

They report that (by their preferred aggregation) **quantitative progress is ~58–66% of the pace** depicted in AI 2027, and they summarize this as “roughly 65%.” ([AI Futures][2])

They also explicitly warn that **aggregating over individual predictions** would overweight compute because compute makes up many rows (7 of the predictions), so they prefer aggregating category means/medians. ([AI Futures][2])

**Independent-research replication criterion:**
Ask researchers to (a) reproduce the pace multipliers per metric, and (b) reproduce at least two aggregation methods:

1. “category means/medians” (their preferred approach) ([AI Futures][2])
2. “individual metrics” (their alternative approach, which they argue is less meaningful) ([AI Futures][2])

### 1.2 Quantitative criteria: the exact metric list they used (with resolution approach)

Everything below is directly from their grading spreadsheet, including baselines, targets, and what counted as “reality.” ([Google Docs][3])

#### A) Benchmarks category (2 metrics)

1. **OSWorld-Verified (mid-2025)**

* Baseline (Apr 2025): ~0.381
* Predicted (mid-2025): 0.65
* “Actual SOTA” (Aug 2025): ~0.608
* Scoring: they “graded by assuming logistic progression” ([Google Docs][3])

2. **SWE-bench Verified (mid-2025)**

* Baseline (Apr 2025): ~0.717
* Predicted (mid-2025): 0.85
* “Actual SOTA” (Aug 2025): ~0.745 (they tie this to “Opus 4.1” in the review narrative) ([Google Docs][3])
* Again: “graded by assuming logistic progression” ([Google Docs][3])

**Independent-research questions to ask:**

* “What is the best public SOTA score at the specified cutoff date, on the specified benchmark variant?”
* “Is logistic interpolation defensible for this benchmark, or is progress lumpy/stepwise?”
* “How sensitive is the pace score to using Aug 2025 vs end-of-mid-2025, and to what you count as SOTA (single model vs scaffolded agent vs ensemble)?”

#### B) Coding time horizon category (2 metrics)

These are based on **METR’s 80% coding time horizon** concept, with careful handling of a methodology version change (TH1.0 → TH1.1).

3. **Coding time horizon, “model trajectory” (late-2025)**

* They treat best current 80% time horizon as “GPT-5.2 at 55 minutes” (TH1.1)
* They apply an adjustment factor to compare the old and new methodology (“we look at when the AI 2027 trajectory reaches 83 minutes”)
* Reported pace: 1.04× ([Google Docs][3])

4. **Coding time horizon, “erroneous graph trajectory” (late-2025)**

* Same real-world anchor, but compared against a trajectory they say was plotted incorrectly in AI 2027 materials
* Reported pace: 0.66× ([Google Docs][3])

**Independent-research questions to ask:**

* “Confirm the benchmark definition (80% time horizon), the suite version, and the best score by date.”
* “Quantify the effect of the TH version change and whether their 3/2 correction is appropriate.”
* “Check whether time horizon is being improved by genuine autonomy vs tool scaffolding; how does that map to their AC milestone?”

#### C) AI software R&D uplift category (2 metrics)

Both of these use a central estimate of “AI software R&D uplift” taken from their *own new model* (“aifuturesmodel.com estimates uplift was 1.12× as of end of 2025”). ([Google Docs][3])

5. **AI software R&D uplift: OpenBrain (late-2025)**

* “Actual” = 1.12× (their model)
* Compared to their internal-company depiction
* Reported pace: **−0.10×** (negative pace) ([Google Docs][3])

6. **AI software R&D uplift: public model (late-2025)**

* “Actual” = 1.12× (their model)
* Compared to their public-model depiction
* Reported pace: 0.44× ([Google Docs][3])

**Independent-research questions to ask:**

* “Is it methodologically valid to resolve an AI 2027 prediction with an estimate from a later model built by the same authors?”
* “What external empirical evidence supports 1.12× by end-2025 (RCTs, surveys, telemetry)?”
* “How should ‘uplift’ separate (i) coding throughput, (ii) research taste, and (iii) experiment/compute bottlenecks?”

#### D) Economic value category (2 metrics)

7. **Annualized revenue of leading AGI company (late-2025)**

* They use OpenAI as the “highest AGI company annualized revenue,” saying ~$20B by end of 2025 ([Google Docs][3])
* Reported pace: 1.16× ([Google Docs][3])

8. **Valuation of leading AGI company (late-2025)**

* They use OpenAI valuation ~$500B (as of Oct 2025) ([Google Docs][3])
* Reported pace: 0.44× ([Google Docs][3])

**Independent-research questions to ask:**

* “How much of valuation is capability vs macro liquidity / investor sentiment?”
* “Do we want revenue and valuation to be capability proxies, or adopt a more direct ‘economic value created by AI systems’ measure?”

#### E) Compute category (7 metrics)

They include 7 compute-related rows, spanning: largest training run FLOPs, China compute budget, global datacenter spend, chip supply (H100-equivalents), leading company compute budget, datacenter capacity, datacenter capex. ([Google Docs][3])

They explicitly highlight that **largest training run compute** is extremely uncertain; they say their best guess is no substantially larger run than GPT‑4.5 (~4e26 FLOP), but uncertainty is huge. ([AI Futures][2])

**Independent-research questions to ask:**

* “What is the best-estimate range for largest training run FLOPs by end-2025?”
* “What’s the best-estimate leading-lab compute budget (FLOPs/month), datacenter power capacity (GW), and capex?”
* “What is the best-estimate China leading player compute budget and access constraints?”
* “How sensitive are these to assumptions like utilization and H100e conversion?”

#### F) Public salience category (1 metric)

9. **% of Americans saying AI is the “most important problem” (late-2025)**

* Uses Gallup “most important problem” polling
* Baseline ~0.38% → predicted 1% → actual 0.63%
* Logistic grading again
* Pace 0.52× ([Google Docs][3])

**Independent-research questions to ask:**

* “Is this a robust proxy for global political salience?”
* “How noisy is the metric at sub-1% levels, and how did they smooth it (rolling average)?”

---

### 1.3 Their qualitative criteria

In the Feb 12, 2026 post they grade AI 2027’s qualitative narrative by:

* quoting scenario text (italicized) and adding a short “seems correct / broadly accurate” judgment
* sometimes noting partial misses (e.g., they think Slack/Teams usage didn’t become especially prominent, despite “spirit of prediction” being right) ([AI Futures][2])

Examples of qualitative items they explicitly judge include: agents appearing and being marketed as personal assistants, struggles to get widespread usage, specialized coding agents transforming work, reliability/cost tradeoffs, faster release cadence, and risk capability signals (bio/hacking). ([AI Futures][2])

**Independent-research replication criterion:**
Before researchers read the authors’ “grading,” have them independently score the same qualitative claims on a rubric like:

* **0 = wrong**, **1 = partly**, **2 = mostly**, **3 = strongly correct**
  …and require short evidence notes.

---

### 1.4 Criteria embedded in their Dec 2025 model update (what changed, and why it matters)

The Dec 31, 2025 update isn’t a “grade sheet,” but it *does* define criteria/assumptions for how they map evidence into timelines/takeoff forecasts:

* They describe a **new unified model** that forecasts milestones like **Automated Coder (AC)** and **superintelligence (ASI)**. ([AI Futures][1])
* They claim it implies **longer timelines to full coding automation** than their earlier AI 2027 models by **~3–5 years**, “in significant part” because they’re now **less bullish on pre-full-automation AI R&D speedups**. ([AI Futures][1])
* They explicitly compare: their *previous* timelines model (median parameters) implied **superhuman coder medians ~2027–2028**, while the new model implies **~2032**, and they attribute much of the difference to improved modeling of AI R&D automation. ([AI Futures][1])

They also define key operational milestones:

* **Automated Coder (AC)** = system that can “fully automate an AGI project’s coding work,” replacing the project’s whole coding staff. ([AI Futures][1])
* **Superhuman Coder (SC)** (from the AI 2027 timelines research) = can do any coding tasks the best AGI-company engineer can do, but “much faster and cheaper.” ([AI 2027][4])

And they describe Stage 1 of the model as:

* start with **METR time-horizon extrapolation** and map a time horizon to AC, then adjust for drivers that could bend the trend down or up:

  * possible slowdowns in compute/labor/data growth due to real-world constraints
  * possible upward bending from increasing AI R&D automation ([AI Futures][1])

For takeoff modeling, they argue their emphasis differs from prior SIE models:

* they focus on AI “research taste” as a key driver of a software intelligence explosion, and they say if an SIE occurs, it will be “primarily driven by improvements in research taste.” ([AI Futures][1])

Finally, they show how the authors themselves adjust “model output” into “all-things-considered” views. For example, Eli lists adjustments like model limitations/unknown unknowns and data bottlenecks, and then gives an explicit AC distribution (p10/p50/p90) as an “all-things-considered” distribution. ([AI Futures][1])

---

## 2) Critical analysis of their review

### 2.1 What’s strong about their review approach

They did several things that are unusually good practice in forecasting:

* **Pre-commitment to falsifiable predictions:** They emphasize that AI 2027 included many concrete quantitative predictions and that they’re now grading them. ([AI Futures][2])
* **Transparent resolution artifacts:** They provide a public spreadsheet with baselines, targets, and resolution notes (including links for many rows). ([Google Docs][3])
* **Separating “model outputs” from “all-things-considered” judgment:** Their Dec 2025 update and forecasts consistently distinguish raw model outputs vs post-model adjustments. ([AI Futures][1])
* **Acknowledging measurement uncertainty explicitly:** Especially on compute (largest training runs), they foreground that the uncertainty is huge and that it’s hard to rule out scale-ups. ([AI Futures][2])
* **Correcting known mistakes:** They explicitly track an “erroneous graph trajectory” vs the correct trajectory for time horizons. ([AI Futures][2])

### 2.2 Where the quantitative “pace multiplier” can mislead

Even if the spreadsheet is transparent, several methodological choices can produce a misleading “single pace number”:

**(1) Logistic interpolation is a strong assumption.**
They repeatedly say “graded by assuming logistic progression” for benchmark-like metrics and public salience. ([Google Docs][3])
But frontier model progress often comes in **step changes** when a new model/scaffold is released, and may show long plateaus. A logistic curve can be a fine hindsight fit, but it can also overstate confidence in “smoothness.”

**(2) Using “SOTA at end of Aug 2025” to grade a “mid 2025” target is defensible but contestable.**
They do this for OSWorld-Verified and SWE-bench Verified. ([Google Docs][3])
That choice (and the cutoff date choice) can swing the pace scores.

**(3) Circularity risk in “AI software R&D uplift.”**
They resolve uplift using the later AI Futures Model’s uplift estimate (“aifuturesmodel.com estimates uplift was 1.12×…”). ([Google Docs][3])
That’s not *automatically wrong*, but it is structurally different from resolving against an external benchmark. If you’re trying to independently test whether the *story* pace was right, resolving with a later internal model introduces feedback loops.

**(4) “Valuation pace” is a noisy signal.**
They interpret valuation being “behind pace” as evidence of slower progress. ([AI Futures][2])
But valuations can lag or lead capabilities due to interest rates, risk appetite, or a shift in how markets price AI capture/competition.

**(5) The compute section is both important and very hard to resolve.**
They say their best guess is no training run bigger than GPT‑4.5 by end of 2025, but also emphasize uncertainty. ([AI Futures][2])
Given that one row (“largest training run”) gets a *very* low pace (0.02×), any aggregation is sensitive to this single uncertain estimate.

### 2.3 The qualitative review is informative, but it’s still subjective

Their qualitative grading has value, but:

* It’s not pre-registered as a rubric, so **confirmation bias** is possible.
* Some judgments depend on ambiguous notions like “spirit of the prediction,” which can be reasonable but makes independent replication harder. ([AI Futures][2])

A robust independent process would require (i) blind scoring, then (ii) evidence adjudication.

### 2.4 The Gary Marcus critique: useful warnings, but also incomplete

Marcus’s core critique is that AI 2027 reads like a thriller crafted to look like science, and that narrative techniques are doing much of the persuasive work. ([Gary Marcus][5])
He also argues they rely too heavily on a “flawed” METR graph and implicitly assume exponentials keep going (his “disco fallacy” framing). ([Gary Marcus][5])

Where this critique is particularly valuable for your gap analysis:

* It highlights a **category error risk**: treating a vivid narrative as if it were a scientific forecast, when the causal “roadmap” may not be fully specified. ([Gary Marcus][5])
* It warns that selectively presented graphs can under-communicate tail risk/uncertainty (he claims the appendix visualization cuts off probability mass after ~2036 even when 90th percentiles are much later). ([Gary Marcus][5])

Where Marcus may be less helpful (for forecasting calibration) is that the authors themselves repeatedly emphasize uncertainty and now provide quantitative self-grading artifacts; dismissing it as “just fiction” can obscure the useful parts (like the explicit prediction list and resolution sheet). ([AI Futures][2])

---

## 3) Our independent assessment as of Feb 2026

### 3.1 What the authors’ own evidence implies (even before we add our judgments)

From their spreadsheet and summary:

* Their preferred overall quantitative pace is **~0.63–0.66**, i.e. roughly **2/3 speed** vs AI 2027. ([AI Futures][2])
* If you look only at their “capabilities indicators” (benchmarks, time horizons, uplift, economic value), the mean is about **0.58** (they report this in the sheet). ([Google Docs][3])
* Within that, there is a **mixed picture**:

  * SWE-bench Verified is far behind their mid-2025 target (very low pace score). ([AI Futures][2])
  * Time horizon looks close to on-pace for their “central” trajectory after adjusting for METR’s suite change. ([AI Futures][2])
  * Revenue is slightly ahead of their depicted pace, while valuation is behind. ([AI Futures][2])

**My read:** even if you fully accept their scoring method, the conclusion is not “AI progress has slowed dramatically.” It’s more like: *some capability indicators (notably SWE-bench) are slower than expected, while time-horizon and compute build-out look closer to expectations, and economic signals are mixed.*

### 3.2 A more conservative “pace range” (still using their data)

If we keep their metric list but adjust interpretation:

* The **R&D uplift rows** are partly driven by baseline re-estimation and reliance on their later model for “actual,” and one is even negative pace. ([Google Docs][3])
* If you compute a “capability indicators” mean **excluding the negative uplift artifact**, you land closer to ~0.68 (this isn’t their published aggregate, but it’s a reasonable sensitivity check). ([Google Docs][3])
* If you also down-weight valuation (high macro noise), you can get capability pace estimates in the **~0.6–0.7** band without doing violence to their own inputs. ([Google Docs][3])

So: **“~0.6–0.7× pace”** is a reasonable independent range *given their own evidence*, with **~0.65×** as a central point.

### 3.3 What this means for the “AI 2027” calendar

The authors do exactly the kind of scaling exercise you’d want:

* AI 2027 depicted takeoff from full coding automation to superintelligence during 2027. ([AI Futures][2])
* If progress were to continue at ~65% of that pace, they suggest the takeoff window shifts later (they give a rough adjusted window, and then an additional adjustment using their AI Futures Model that incorporates slowdown effects). ([AI Futures][2])

But the bigger shift is that **their Dec 2025 model update lengthened the timeline to full coding automation**:

* They say the AI Futures Model predicts **~3–5 years longer** to “full coding automation” than their previous AI 2027 models. ([AI Futures][1])
* They explicitly compare SC medians: old model ~2027–2028 vs new model ~2032, attributing much of the change to improved modeling of AI R&D automation. ([AI Futures][1])

And their accessible current forecasts align with that “later-than-2027” picture:

* Daniel’s all-things-considered forecast for **Automated Coder** has a **median around Dec 2029** (with wide uncertainty). ([AI Futures Model][6])
* Eli’s all-things-considered AC distribution (as described in the Dec update) has **p50 around 2032.5** (also wide). ([AI Futures][1])

**Independent bottom line (calibrated to their current work):**
If you want a date-level statement, the most defensible one *from these materials alone* is:

* The “full coding automation by 2027” cadence looks **ahead of the median** view even for the authors now.
* The best-supported “central tendency” is **late-2020s to early-2030s** for AC, with extremely fat right tails. ([AI Futures Model][6])

### 3.4 Where I think their *model update* is directionally right

Their Dec 2025 update claims the biggest reason they got longer is being “less bullish on pre-full-automation AI R&D speedups.” ([AI Futures][1])

That seems directionally consistent with:

* the ambiguity and mixed evidence on “uplift” (they themselves say the extent of R&D acceleration is unclear) ([AI Futures][1])
* the fact they’re leaning on better-structured modeling of R&D automation and explicitly separating model output from “unknown unknowns” adjustments. ([AI Futures][1])

It also cleanly addresses one of the strongest generic failure modes of short-timeline stories: assuming early-stage tools produce near-term *massive* end-to-end R&D acceleration.

### 3.5 Where I think their *grading conclusions* are most fragile

If I had to pick the top 3 fragilities that independent researchers should stress-test first:

1. **Benchmark choice and resolution windows** (mid-2025 targets resolved with Aug-2025 SOTA, logistic smoothing). ([Google Docs][3])
2. **R&D uplift measurement** (self-resolving with their later model estimate; negative pace artifact). ([Google Docs][3])
3. **Compute “largest training run”** (high-impact row with very high uncertainty). ([AI Futures][2])

---

## 4) A practical “gap analysis” kit you can hand to independent researchers

### 4.1 Deliverable format to request from researchers

Ask each researcher/team to produce a 1–2 page memo plus an appendix table with these columns:

* **Metric / claim** (exactly as named in the spreadsheet)
* **Operational definition** (what counts, what doesn’t)
* **Cutoff date** (and why)
* **Best estimate** (with uncertainty range)
* **Pace multiplier** (if applicable)
* **Key evidence** (primary sources preferred)
* **Disagreements with authors** (and why)
* **Confidence level** (low/med/high)

### 4.2 The criteria list to include verbatim in your RFP

Below is the list of criteria that match the authors’ review inputs (so you can do clean comparisons):

**Quantitative criteria (reproduce pace multipliers):** ([Google Docs][3])

* OSWorld-Verified SOTA vs AI 2027 target
* SWE-bench Verified SOTA vs AI 2027 target
* METR 80% coding time horizon vs AI 2027 “model trajectory”
* METR 80% coding time horizon vs AI 2027 “erroneous graph trajectory”
* AI software R&D uplift (internal/OpenBrain) at end-2025
* AI software R&D uplift (public) at end-2025
* Leading AI company annualized revenue at end-2025
* Leading AI company valuation at end-2025
* Largest training run FLOPs by end-2025
* China leading company compute budget by end-2025
* Global AI datacenter spend by end-2025
* Global H100-equivalents available by end-2025
* Leading AI company compute budget (FLOPs/month) by end-2025
* Leading AI company datacenter capacity (GW) by end-2025
* Leading AI company datacenter capex by end-2025
* % of Americans citing AI as “most important problem” by end-2025

**Qualitative criteria (score with rubric):** ([AI Futures][2])

* emergence of “computer-using agents” marketed as personal assistants
* degree of mainstream adoption vs “struggle for widespread usage”
* reality of specialized coding/research agents transforming workflows
* reliability/cost characteristics of agents
* whether top labs’ “race” is closer/tighter than depicted
* whether “finishes training” is increasingly a misnomer (frequent updates)
* how “hacking assistance” and “bio risk assistance” are tracking

**Model-structure criteria (for auditing the Dec 2025 update):**

* Does extrapolating METR time horizons map plausibly to AC? ([AI Futures][1])
* Are compute/labor slowdowns and R&D automation the right dominant bends? ([AI Futures][1])
* Is “research taste” the right central driver for takeoff? ([AI Futures][1])
* Do their “3–5 years longer” conclusions follow from parameter updates, or from structural model choices? ([AI Futures][1])

### 4.3 A simple way to score “gap” once you get memos back

For each metric, compute:

* **Gap (absolute):** independent best estimate − authors best estimate
* **Gap (standardized):** (independent − authors) / independent uncertainty width
* **Direction:** faster vs slower than AI 2027 pace
* **Root cause tag:** measurement choice / definition mismatch / data source mismatch / conceptual disagreement

Then summarize:

* **“Pace” gap:** do independent teams get ~0.6–0.7× too, or something materially different? ([AI Futures][2])
* **“Driver” gap:** do they agree the main revision should come from R&D automation assumptions? ([AI Futures][1])

---

## 5) One-page synthesis you can use internally

* The authors now claim: **AI 2027’s quantitative 2025 predictions were too fast**, with reality running at **~2/3 pace**, and they’ve updated to a model that pushes **full coding automation later** largely by being less bullish on early R&D automation. ([AI Futures][2])
* Their own artifacts are unusually transparent (spreadsheet + explicit pace method), but the “pace” result is sensitive to (i) logistic interpolation assumptions, (ii) cutoff date choices, (iii) self-resolving uplift with their newer model, and (iv) highly uncertain compute estimates. ([Google Docs][3])
* An independent read of their own data supports a **~0.6–0.7×** pace range without much strain. ([Google Docs][3])
* Their *current* medians for AC (coding fully automated) are **late-2020s to early-2030s** rather than 2027, with very wide tails. ([AI Futures Model][6])
* Gary Marcus’s critique is a reminder that narrative vividness can substitute for a roadmap, and that exponential extrapolation can be rhetorically powerful; it’s worth treating AI 2027 as a scenario framework + prediction bundle rather than “a single forecast.” ([Gary Marcus][5])

---

If you want, I can also produce a **ready-to-send independent-researcher questionnaire** (in a format your team can drop into an email/RFP), using the exact metric definitions above and a scoring sheet aligned to their spreadsheet categories—so you can directly compute deltas and do the gap analysis with minimal cleanup.

[1]: https://blog.ai-futures.org/p/ai-futures-model-dec-2025-update "https://blog.ai-futures.org/p/ai-futures-model-dec-2025-update"
[2]: https://blog.ai-futures.org/p/grading-ai-2027s-2025-predictions "https://blog.ai-futures.org/p/grading-ai-2027s-2025-predictions"
[3]: https://docs.google.com/spreadsheets/d/1Ncol1SYIeLhGKdOUHKSYPxCTjkUwQVxkIbp393hveyA/edit?gid=145629229 "https://docs.google.com/spreadsheets/d/1Ncol1SYIeLhGKdOUHKSYPxCTjkUwQVxkIbp393hveyA/edit?gid=145629229"
[4]: https://ai-2027.com/research/timelines-forecast "https://ai-2027.com/research/timelines-forecast"
[5]: https://garymarcus.substack.com/p/the-ai-2027-scenario-how-realistic "https://garymarcus.substack.com/p/the-ai-2027-scenario-how-realistic"
[6]: https://www.aifuturesmodel.com/forecast/daniel-01-26-26 "https://www.aifuturesmodel.com/forecast/daniel-01-26-26"

