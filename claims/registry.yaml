# Claim Registry
# Exported from LanceDB on 2026-01-24
# Total claims: 235
# Total chains: 4

counters:
  TECH: 91
  RESOURCE: 6
  LABOR: 13
  ECON: 19
  GOV: 32
  SOC: 3
  TRANS: 15
  META: 23
  GEO: 1
  RISK: 5
  INST: 3
claims:
  ECON-2025-001:
    text: Neoliberalism evolved into a model of state hollowing and asset-stripping
      via outsourcing to private-sector entities owned by elites, and today’s ruling
      elites lack the institutional memory to adapt to major change.
    type: '[T]'
    domain: ECON
    evidence_level: E5
    credence: 0.30000001192092896
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  ECON-2025-002:
    text: Platform capitalism increasingly sells subscription services rather than
      owned goods, enabling rent extraction by degrading quality and raising prices
      (“enshittification”).
    type: '[T]'
    domain: ECON
    evidence_level: E4
    credence: 0.4000000059604645
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  ECON-2026-001:
    text: Compute + energy + chips becoming primary chokepoints for economic power
    type: '[T]'
    domain: ECON
    evidence_level: E3
    credence: 0.75
    source_ids:
    - gpt-2026-01-18-hottakes
    - gpt-2026-01-18-fiction-hottakes
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts: []
    depends_on:
    - TECH-2026-001
    - RESOURCE-2026-001
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  ECON-2026-002:
    text: Bottleneck control, not model access, determines 'feudal' vs 'abundance'
      outcomes
    type: '[T]'
    domain: ECON
    evidence_level: E4
    credence: 0.699999988079071
    source_ids:
    - gpt-2026-01-18-hottakes
    - gpt-2026-01-18-fiction-hottakes
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts: []
    depends_on:
    - ECON-2026-001
    - TECH-2026-004
    modified_by: []
    part_of_chain: CHAIN-2026-003
    version: 1
    last_updated: '2026-01-18'
  ECON-2026-003:
    text: Post-labor → permanent underclass is 'default' outcome
    type: '[P]'
    domain: ECON
    evidence_level: E5
    credence: 0.30000001192092896
    source_ids:
    - teortaxes-2026-thread
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts:
    - ECON-2026-011
    - GOV-2026-004
    depends_on:
    - LABOR-2026-002
    - LABOR-2026-003
    - ECON-2026-012
    modified_by: []
    part_of_chain: CHAIN-2026-001
    version: 1
    last_updated: '2026-01-18'
    notes: Assumes no redistribution; politically contestable
  ECON-2026-004:
    text: Technology diffusion + commoditization historically breaks concentration
    type: '[T]'
    domain: ECON
    evidence_level: E2
    credence: 0.699999988079071
    source_ids:
    - historical-pattern
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts:
    - ECON-2026-003
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  ECON-2026-005:
    text: The premise that 'only billionaires can afford AI cognition' is false for
      most domains
    type: '[T]'
    domain: ECON
    evidence_level: E2
    credence: 0.75
    source_ids:
    - gpt-2026-01-18-hottakes
    - gpt-2026-01-18-fiction-hottakes
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - ECON-2026-002
    contradicts: []
    depends_on:
    - TECH-2026-004
    modified_by: []
    part_of_chain: CHAIN-2026-003
    version: 1
    last_updated: '2026-01-18'
  ECON-2026-006:
    text: Inference at scale may centralize around power/datacenters even if models
      commoditize
    type: '[H]'
    domain: ECON
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - gpt-2026-01-18-hottakes
    - gpt-2026-01-18-fiction-hottakes
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - ECON-2026-001
    contradicts: []
    depends_on:
    - RESOURCE-2026-001
    modified_by: []
    part_of_chain: CHAIN-2026-003
    version: 1
    last_updated: '2026-01-18'
    notes: 'Key uncertainty: efficiency/edge inference vs hyperscaler vertical integration
      and grid constraints'
  ECON-2026-007:
    text: Monopolistic tech firms with saturated markets repeatedly hype new 'bubble'
      narratives (crypto/NFTs/metaverse/AI) to preserve growth-stock valuations when
      organic growth slows
    type: '[T]'
    domain: ECON
    evidence_level: E4
    credence: 0.44999998807907104
    source_ids:
    - doctorow-2026-reverse-centaur
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - TRANS-2026-003
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  ECON-2026-008:
    text: AI tooling increases the feasible output of small teams/creators, increasing
      the value of mechanisms that allocate attention and funding to creators
    type: '[H]'
    domain: ECON
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - yegge-2026-bags-creator-economy
    - yegge-2026-future-coding-agents
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - LABOR-2026-009
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  ECON-2026-009:
    text: As internal software becomes easier to rebuild, some SaaS categories face
      stronger build-vs-buy pressure; defensible vendors are those with deep accumulated
      infra/know-how that remains costly to recreate
    type: '[H]'
    domain: ECON
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - yegge-2025-six-new-tips-coding-agents
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on:
    - LABOR-2026-011
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  ECON-2026-010:
    text: '''Creator explosion'' forecast: millions of independent creators appear
      once ''everyone can vibe code'' (claimed as a near-term function of ~2 frontier-model
      upgrades), forcing new discovery/market institutions'
    type: '[P]'
    domain: ECON
    evidence_level: E5
    credence: 0.30000001192092896
    source_ids:
    - yegge-2026-bags-creator-economy
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - ECON-2026-008
    contradicts: []
    depends_on:
    - LABOR-2026-012
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  ECON-2026-011:
    text: Post-labor requires some income distribution mechanism or consumption collapses
    type: '[T]'
    domain: ECON
    evidence_level: E3
    credence: 0.800000011920929
    source_ids:
    - macro-identity
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - GOV-2026-004
    contradicts:
    - ECON-2026-003
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  ECON-2026-012:
    text: Labor economically irrelevant → no consumer purchasing power (absent redistribution)
    type: '[A]'
    domain: ECON
    evidence_level: E4
    credence: 0.30000001192092896
    source_ids:
    - teortaxes-2026-thread
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - ECON-2026-003
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: CHAIN-2026-001
    version: 1
    last_updated: '2026-01-18'
    notes: Weakest-link assumption in the permanent underclass chain; collapses if
      dividends/transfers/public provision scale
  ECON-2026-013:
    text: A world of individual fortress-building (strategic autonomy pursued separately)
      will be poorer, more fragile, and less sustainable than collective resilience
      investment
    type: '[H]'
    domain: ECON
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  ECON-2026-014:
    text: Bridging the Trans-Pacific Partnership and European Union would create a
      new trading bloc of 1.5 billion people
    type: '[F]'
    domain: ECON
    evidence_level: E4
    credence: 0.8999999761581421
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-21'
    notes: 'QUESTIONABLE: Math check - EU (~447M) + CPTPP (~500M) = ~950M, not 1.5B.
      Either claim includes future expansion or uses different bloc composition. Needs
      clarification on what constitutes the ''bridged'' bloc.'
  ECON-2026-015:
    text: OpenAI argues its business model should scale with the value that intelligence
      delivers, so monetization mechanisms should capture value in proportion to outcomes
      produced rather than being fixed or purely cost-based.
    type: '[T]'
    domain: ECON
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - openai-value-intelligence
    first_extracted: '2026-01-22'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-22'
  ECON-2026-016:
    text: OpenAI reports revenue scaled roughly 3× year over year from 2023 to 2025,
      from about $2B in annual recurring revenue (ARR) in 2023 to $6B in 2024 and
      more than $20B in 2025.
    type: '[F]'
    domain: ECON
    evidence_level: E4
    credence: 0.6499999761581421
    source_ids:
    - openai-value-intelligence
    first_extracted: '2026-01-22'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-22'
  ECON-2026-017:
    text: OpenAI asserts that if it had access to more compute during 2023–2025, it
      would have achieved faster customer adoption and monetization during those periods.
    type: '[C]'
    domain: ECON
    evidence_level: E5
    credence: 0.550000011920929
    source_ids:
    - openai-value-intelligence
    first_extracted: '2026-01-22'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-22'
  ECON-2026-018:
    text: As AI moves into domains like scientific research, drug discovery, energy
      systems, and financial modeling, new economic models (licensing, IP-based agreements,
      and outcome-based pricing) will emerge to share in the value created.
    type: '[P]'
    domain: ECON
    evidence_level: E6
    credence: 0.44999998807907104
    source_ids:
    - openai-value-intelligence
    first_extracted: '2026-01-22'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-22'
  ECON-2026-019:
    text: Organizations may rationally pay on the order of $1k–$3k/month for a robust
      agent orchestrator if it yields ~2–3× productivity on senior developers; willingness-to-pay
      scales with local developer salaries.
    type: '[H]'
    domain: ECON
    evidence_level: E5
    credence: 0.5
    source_ids:
    - appleton-2026-gastown-agent-patterns
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  GEO-2025-001:
    text: The energy transition is producing major political consequences—from bribery
      and corruption up to open warfare—and post-oil geopolitics will differ substantially
      from the oil-centric post-1945 era.
    type: '[T]'
    domain: GEO
    evidence_level: E5
    credence: 0.3499999940395355
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  GOV-2026-001:
    text: Property rights will become arbitrary/confiscatable in neofeudal regime
    type: '[H]'
    domain: GOV
    evidence_level: E6
    credence: 0.25
    source_ids:
    - hotz-2026-three-minutes
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-18'
    notes: Contradicts @teortaxesTex 'equity escape' thesis (convert wealth to dividends
      to survive)
  GOV-2026-002:
    text: Elites will choose genocide over UBI once labor is unnecessary
    type: '[S]'
    domain: GOV
    evidence_level: E6
    credence: 0.10000000149011612
    source_ids:
    - xenoimpulse-2026-thread
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts:
    - GOV-2026-004
    - GOV-2026-005
    depends_on:
    - LABOR-2026-003
    - GOV-2026-008
    - GOV-2026-009
    - GOV-2026-010
    - GOV-2026-011
    modified_by: []
    part_of_chain: CHAIN-2026-002
    version: 1
    last_updated: '2026-01-18'
    notes: Multiple weak premises; ignores cheaper alternatives; coordination nightmare
  GOV-2026-003:
    text: US elites 'ahead of curve' and coordinating on post-labor strategy
    type: '[S]'
    domain: GOV
    evidence_level: E6
    credence: 0.20000000298023224
    source_ids:
    - teortaxes-2026-thread
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
    notes: Unfalsifiable
  GOV-2026-004:
    text: Buying stability often cheaper than enforcing it in high-automation world
    type: '[T]'
    domain: GOV
    evidence_level: E3
    credence: 0.6499999761581421
    source_ids:
    - gpt-2026-01-18-hottakes
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts:
    - GOV-2026-002
    - ECON-2026-003
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  GOV-2026-005:
    text: Inter-elite + interstate competition prevents unified oligarchic coalition
    type: '[T]'
    domain: GOV
    evidence_level: E3
    credence: 0.699999988079071
    source_ids:
    - gpt-2026-01-18-hottakes
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts:
    - GOV-2026-002
    - GOV-2026-003
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  GOV-2026-006:
    text: Multipolarity reduces single-coalition doom but increases security competition
    type: '[T]'
    domain: GOV
    evidence_level: E4
    credence: 0.6499999761581421
    source_ids:
    - gpt-2026-01-18-hottakes
    - gpt-2026-01-18-fiction-hottakes
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  GOV-2026-007:
    text: Japan/EU governance styles suggest 'civilized equilibrium' paths exist
    type: '[H]'
    domain: GOV
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - japan-ai-guidelines-2025
    - eu-ai-act
    - gpt-2026-01-18-hottakes
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts:
    - ECON-2026-003
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-18'
  GOV-2026-008:
    text: Even post-labor, humans remain a threat vector (revolt, sabotage, voting)
    type: '[F]'
    domain: GOV
    evidence_level: E1
    credence: 0.8999999761581421
    source_ids:
    - xenoimpulse-2026-thread
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - GOV-2026-002
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: CHAIN-2026-002
    version: 1
    last_updated: '2026-01-18'
  GOV-2026-009:
    text: AI/automation makes coercion cheap and reliable at scale
    type: '[H]'
    domain: GOV
    evidence_level: E5
    credence: 0.25
    source_ids:
    - xenoimpulse-2026-thread
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - GOV-2026-002
    contradicts:
    - GOV-2026-004
    - GOV-2026-005
    depends_on: []
    modified_by: []
    part_of_chain: CHAIN-2026-002
    version: 1
    last_updated: '2026-01-18'
    notes: 'Contested: coercion historically fragile/expensive; automation introduces
      new points of failure'
  GOV-2026-010:
    text: Elites can coordinate on extreme solutions (including mass extermination)
      in a post-labor world
    type: '[H]'
    domain: GOV
    evidence_level: E6
    credence: 0.15000000596046448
    source_ids:
    - xenoimpulse-2026-thread
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - GOV-2026-002
    contradicts:
    - GOV-2026-005
    depends_on: []
    modified_by: []
    part_of_chain: CHAIN-2026-002
    version: 1
    last_updated: '2026-01-18'
    notes: 'Weakest link: N-player coordination problem with strong incentives to
      defect'
  GOV-2026-011:
    text: Moral norms and legitimacy stop constraining elite behavior in a post-labor
      world
    type: '[H]'
    domain: GOV
    evidence_level: E6
    credence: 0.20000000298023224
    source_ids:
    - xenoimpulse-2026-thread
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - GOV-2026-002
    contradicts:
    - GOV-2026-004
    depends_on: []
    modified_by: []
    part_of_chain: CHAIN-2026-002
    version: 1
    last_updated: '2026-01-18'
    notes: 'Contested: regimes typically invest in legitimacy because it is cheaper
      than pure coercion'
  GOV-2026-012:
    text: US behavior (e.g., Greenland/Venezuela bargaining) can be interpreted as
      coherent resource-extraction strategy that overrides alliance norms
    type: '[H]'
    domain: GOV
    evidence_level: E6
    credence: 0.20000000298023224
    source_ids:
    - teortaxes-2026-greenland-endgame
    first_extracted: '2026-01-18'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  GOV-2026-013:
    text: Legal/formal ownership claims can reduce the cost of coercive control by
      making anti-aggressor coalitions harder to form
    type: '[T]'
    domain: GOV
    evidence_level: E3
    credence: 0.550000011920929
    source_ids:
    - teortaxes-2026-greenland-endgame
    first_extracted: '2026-01-18'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  GOV-2026-014:
    text: Europe’s feasible response to US territorial/resource pressure is delay/placation
      plus diversification and partial rearmament, not direct confrontation
    type: '[H]'
    domain: GOV
    evidence_level: E6
    credence: 0.25
    source_ids:
    - teortaxes-2026-greenland-endgame
    first_extracted: '2026-01-18'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  GOV-2026-015:
    text: A faction of US tech/policy elites holds a longtermist 'endgame' worldview
      motivating resource competition (e.g., Musk/Thiel/Karp/Matheny)
    type: '[H]'
    domain: GOV
    evidence_level: E6
    credence: 0.15000000596046448
    source_ids:
    - teortaxes-2026-greenland-endgame
    first_extracted: '2026-01-18'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  GOV-2026-016:
    text: Google has ~90% search market share (global, depending on metric/timeframe),
      consistent with near-monopoly conditions
    type: '[F]'
    domain: GOV
    evidence_level: E2
    credence: 0.8999999761581421
    source_ids:
    - doctorow-2026-reverse-centaur
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - ECON-2026-007
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  GOV-2026-017:
    text: Google pays Apple on the order of ~$20bn/year for default search placement
      (Safari/iOS), reinforcing distribution control and monopoly dynamics
    type: '[F]'
    domain: GOV
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - doctorow-2026-reverse-centaur
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - ECON-2026-007
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  GOV-2026-018:
    text: Expanding copyright to restrict model training would primarily benefit incumbent
      media/platform monopolies; creator-friendly leverage is labor-market power (e.g.,
      sectoral bargaining)
    type: '[H]'
    domain: GOV
    evidence_level: E4
    credence: 0.4000000059604645
    source_ids:
    - doctorow-2026-reverse-centaur
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
    notes: 'Institution-dependent: hinges on market concentration and bargaining structures
      in creative industries'
  GOV-2026-019:
    text: Open-source projects will respond to AI-generated noise with stricter contribution
      norms (e.g., prompt disclosure, gating, or restricting drive-by PRs)
    type: '[H]'
    domain: GOV
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - ronacher-2026-agent-psychosis
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on:
    - LABOR-2026-007
    modified_by: []
    part_of_chain: CHAIN-2026-004
    version: 2
    last_updated: '2026-01-22'
  GOV-2026-020:
    text: Shenzhen Metro signaled limits to support for Vanke (no 'unlimited guarantees'
      / beyond its 'risk comfort zone')
    type: '[H]'
    domain: GOV
    evidence_level: E5
    credence: 0.4000000059604645
    source_ids:
    - perera-2026-chinas-trillion-dollar-illusion
    - perera-2026-china-trade-surplus-collapse-thread
    first_extracted: '2026-01-20'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-20'
    notes: Exact quoted language not independently verified in this repo; treated
      as a hypothesis pending primary-source confirmation
  GOV-2026-021:
    text: The international system is experiencing a structural rupture, not a gradual
      transition, in the rules-based order
    type: '[T]'
    domain: GOV
    evidence_level: E5
    credence: 0.550000011920929
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-21'
    notes: 'SUPPORTED: Greenland crisis provides empirical evidence - US listed by
      Danish intelligence as national security threat for first time alongside Russia/China.
      Wikipedia sources corroborate ''rupture'' thesis.'
  GOV-2026-022:
    text: Great powers are systematically using economic integration as weapons of
      coercion through tariffs, financial infrastructure, and supply chain manipulation
    type: '[T]'
    domain: GOV
    evidence_level: E4
    credence: 0.75
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-21'
    notes: 'SUPPORTED: Greenland crisis demonstrates US using tariff threats as leverage
      against allies; trade war with EU triggered January 2026. Wikipedia sources
      support economic coercion thesis.'
  GOV-2026-023:
    text: Middle powers negotiating bilaterally with hegemons necessarily negotiate
      from a position of weakness and accept subordination
    type: '[T]'
    domain: GOV
    evidence_level: E5
    credence: 0.6499999761581421
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  GOV-2026-024:
    text: Middle power coalitions can create meaningful counterweight to great power
      dominance through collective action
    type: '[H]'
    domain: GOV
    evidence_level: E5
    credence: 0.4000000059604645
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  GOV-2026-025:
    text: Hegemons cannot continually monetize their relationships without allies
      diversifying to reduce vulnerability
    type: '[T]'
    domain: GOV
    evidence_level: E4
    credence: 0.699999988079071
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  GOV-2026-026:
    text: Canada is doubling its defence spending by the end of this decade (by ~2030)
    type: '[P]'
    domain: GOV
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-21'
    notes: 'PARTIALLY VERIFIED: Wikipedia confirms ''sharp increase in defence spending''
      announced by Carney government. Specific ''doubling by end of decade'' target
      not verified - needs Canadian budget source.'
  GOV-2026-027:
    text: The post-WWII rules-based international order is not returning to its previous
      form
    type: '[P]'
    domain: GOV
    evidence_level: E5
    credence: 0.699999988079071
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  GOV-2026-028:
    text: Canada has signed 12 trade and security deals across four continents in
      six months (as of January 2026)
    type: '[F]'
    domain: GOV
    evidence_level: E4
    credence: 0.8500000238418579
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-21'
    notes: 'UNVERIFIED: Could not find enumeration of 12 specific trade/security deals.
      Wikipedia confirms active diplomacy but no specific count. Needs official Canadian
      government press releases to verify.'
  GOV-2026-029:
    text: Canada has concluded new strategic partnerships with China and Qatar (as
      of January 2026)
    type: '[F]'
    domain: GOV
    evidence_level: E4
    credence: 0.8500000238418579
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-21'
    notes: 'PARTIALLY VERIFIED: Wikipedia confirms Carney government ''oversaw improvement
      in relations with China'' but specific ''strategic partnership'' details not
      found. Needs official Canadian government source.'
  GOV-2026-030:
    text: Canada strongly opposes tariffs related to Greenland and calls for focused
      talks on Arctic security and prosperity
    type: '[F]'
    domain: GOV
    evidence_level: E4
    credence: 0.949999988079071
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-21'
    notes: 'VERIFIED: Wikipedia Greenland crisis confirms Canada opposition to US
      tariffs related to Greenland; Canada listed as supporting party in Operation
      Arctic Endurance coalition. See wikipedia-greenland-crisis.'
  GOV-2026-031:
    text: Canada firmly supports Greenland and Denmark's unique right to determine
      Greenland's future and opposes US threats regarding Greenland
    type: '[F]'
    domain: GOV
    evidence_level: E4
    credence: 0.949999988079071
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-21'
    notes: 'VERIFIED: Wikipedia confirms Denmark and Greenland reject US takeover;
      85% Greenlanders oppose; Danish govt confirms troops would defend territory;
      Article 5 invocation discussed. See wikipedia-greenland-crisis.'
  GOV-2026-032:
    text: Canada has agreed a comprehensive strategic partnership with the EU including
      joining SAFE (European defence procurement arrangements)
    type: '[F]'
    domain: GOV
    evidence_level: E4
    credence: 0.8500000238418579
    source_ids:
    - carney-2026-davos-wef-speech
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-21'
    notes: 'UNVERIFIED: No Wikipedia article exists for EU SAFE programme. May be
      too recent for coverage. Needs EU Commission or Canadian government source to
      verify participation in European defence procurement.'
  INST-2025-001:
    text: At Google, a team other than the core Gemini/DeepMind builders modified
      the system’s behavior in ways that produced historically inaccurate “diversity
      enforced” outputs.
    type: '[F]'
    domain: INST
    evidence_level: E6
    credence: 0.20000000298023224
    source_ids:
    - jre-2404-elon-musk-2025-ai-woke-mind-virus
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on:
    - TECH-2025-004
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  INST-2026-001:
    text: OpenAI describes a multi-tier monetization system including consumer subscriptions,
      workplace subscriptions, usage-based API pricing tied to production workloads,
      and extensions into commerce and advertising that are intended to feel native
      to the user experience.
    type: '[F]'
    domain: INST
    evidence_level: E4
    credence: 0.8500000238418579
    source_ids:
    - openai-value-intelligence
    first_extracted: '2026-01-22'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-22'
  INST-2026-002:
    text: OpenAI shifted from relying on a single compute provider to working with
      providers across a diversified ecosystem, increasing resilience and enabling
      compute certainty for planning and financing.
    type: '[F]'
    domain: INST
    evidence_level: E4
    credence: 0.75
    source_ids:
    - openai-value-intelligence
    first_extracted: '2026-01-22'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-22'
  INST-2026-003:
    text: OpenAI manages compute as an actively managed portfolio, training frontier
      models on premium hardware when capability is prioritized and serving high-volume
      workloads on lower-cost infrastructure when efficiency is prioritized.
    type: '[F]'
    domain: INST
    evidence_level: E4
    credence: 0.699999988079071
    source_ids:
    - openai-value-intelligence
    first_extracted: '2026-01-22'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-22'
  LABOR-2023-001:
    text: 'In a controlled experiment (May–Jun 2022) with 95 professional developers,
      the group with access to GitHub Copilot completed an HTTP-server implementation
      task in JavaScript 55.8% faster than the control group (95% CI: 21–89%).'
    type: '[F]'
    domain: LABOR
    evidence_level: E2
    credence: 0.8999999761581421
    source_ids:
    - peng-2023-copilot-productivity
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  LABOR-2023-002:
    text: In the same experiment, Copilot’s measured productivity gains were larger
      for less-experienced developers, older programmers, and those who program more
      hours per day.
    type: '[F]'
    domain: LABOR
    evidence_level: E2
    credence: 0.699999988079071
    source_ids:
    - peng-2023-copilot-productivity
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  LABOR-2024-001:
    text: In ANZ Bank’s Copilot experiment, the Copilot group completed the coding
      challenges about 42.36% faster on average than the control group (based on mean
      time spent per problem).
    type: '[F]'
    domain: LABOR
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - chatterjee-2024-anz-copilot-study
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  LABOR-2024-002:
    text: In ANZ Bank’s Copilot experiment, Copilot-group solutions had about a 12.86%
      higher unit test success ratio than control-group solutions, but the difference
      was not statistically significant.
    type: '[F]'
    domain: LABOR
    evidence_level: E2
    credence: 0.699999988079071
    source_ids:
    - chatterjee-2024-anz-copilot-study
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  LABOR-2026-001:
    text: If AI substitutes human labor in most tasks, wages fall and capital share
      rises
    type: '[T]'
    domain: LABOR
    evidence_level: E3
    credence: 0.8500000238418579
    source_ids:
    - standard-economics
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - LABOR-2026-003
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  LABOR-2026-002:
    text: Full automation of human labor by 2035-2045
    type: '[P]'
    domain: LABOR
    evidence_level: E5
    credence: 0.3499999940395355
    source_ids:
    - teortaxes-2026-thread
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: CHAIN-2026-001
    version: 2
    last_updated: '2026-01-18'
    notes: Timeline speculative; 'full' undefined; ignores task recomposition per
      Acemoglu-Restrepo (2019)
  LABOR-2026-003:
    text: Full automation → labor becomes economically irrelevant (labor share approaches
      ~0)
    type: '[T]'
    domain: LABOR
    evidence_level: E4
    credence: 0.4000000059604645
    source_ids:
    - teortaxes-2026-thread
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - ECON-2026-003
    - GOV-2026-002
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: CHAIN-2026-001
    version: 1
    last_updated: '2026-01-18'
    notes: 'Contested: ignores task recomposition and human-AI complementarity; ''economically
      irrelevant'' often undefined'
  LABOR-2026-004:
    text: Full human labor automation occurs within at most a single generation (possibly
      within a few years)
    type: '[P]'
    domain: LABOR
    evidence_level: E6
    credence: 0.15000000596046448
    source_ids:
    - teortaxes-2026-greenland-endgame
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
    notes: Rhetorically emphatic; timelines undefined; treated as low-confidence prediction
  LABOR-2026-005:
    text: Workplace AI is often deployed to intensify surveillance/control of workers
      (creating 'reverse centaurs'), rather than to augment worker autonomy
    type: '[T]'
    domain: LABOR
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - doctorow-2026-reverse-centaur
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  LABOR-2026-006:
    text: For most high-wage jobs, 'AI can do your job' is false as an end-to-end
      replacement claim; attempted substitution pushes humans into accountability-sink
      roles and can degrade quality
    type: '[H]'
    domain: LABOR
    evidence_level: E4
    credence: 0.5
    source_ids:
    - doctorow-2026-reverse-centaur
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
    notes: Primarily a near-term operational claim; does not preclude long-run task
      substitution
  LABOR-2026-007:
    text: Cheap-to-generate AI-assisted issues/PRs create a harsh review asymmetry
      (minutes to produce vs much longer to evaluate), increasing maintainer burden
      and conflict
    type: '[T]'
    domain: LABOR
    evidence_level: E4
    credence: 0.6000000238418579
    source_ids:
    - ronacher-2026-agent-psychosis
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - LABOR-2026-009
    - GOV-2026-019
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: CHAIN-2026-004
    version: 2
    last_updated: '2026-01-22'
  LABOR-2026-008:
    text: AI agents are a productivity boost but become 'slop machines' when oversight/quality
      gates are relaxed; outcomes depend on supervision and verification
    type: '[T]'
    domain: LABOR
    evidence_level: E4
    credence: 0.6499999761581421
    source_ids:
    - ronacher-2026-agent-psychosis
    - yegge-2026-welcome-gas-town
    - yegge-2026-future-coding-agents
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - LABOR-2026-009
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-22'
  LABOR-2026-009:
    text: As artifact generation becomes cheaper, the scarce input shifts toward verification/integration
      attention (review, tests, and domain judgment)
    type: '[T]'
    domain: LABOR
    evidence_level: E3
    credence: 0.699999988079071
    source_ids:
    - ronacher-2026-agent-psychosis
    - yegge-2026-future-coding-agents
    - yegge-2026-gas-town-emergency-user-manual
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - GOV-2026-019
    contradicts: []
    depends_on:
    - LABOR-2026-007
    modified_by: []
    part_of_chain: CHAIN-2026-004
    version: 2
    last_updated: '2026-01-22'
  LABOR-2026-010:
    text: As agentic throughput rises, coordination/merge becomes a binding constraint;
      teams respond with extreme coordination simplifications (e.g., 'one engineer
      per repo') and continuous broadcast norms
    type: '[H]'
    domain: LABOR
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - yegge-2026-future-coding-agents
    - yegge-2026-gas-town-emergency-user-manual
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - TRANS-2026-004
    contradicts: []
    depends_on:
    - LABOR-2026-009
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  LABOR-2026-011:
    text: 'In many contexts, ''rewrite'' becomes cheaper than ''repair'': tests and
      subsystems are treated as disposable and regenerated, shifting maintenance norms'
    type: '[H]'
    domain: LABOR
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - yegge-2025-six-new-tips-coding-agents
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - ECON-2026-009
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  LABOR-2026-012:
    text: '''Everyone codes'' forecast: as agentic tooling diffuses, many non-engineering
      roles write or modify software, and ''vibe coding'' becomes a default expectation
      for programming jobs'
    type: '[H]'
    domain: LABOR
    evidence_level: E4
    credence: 0.44999998807907104
    source_ids:
    - yegge-2025-cheese-wars-rise-vibe-coder
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - ECON-2026-010
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  LABOR-2026-013:
    text: At high scale, agentic coding shifts the binding constraint from implementation
      time to human design, planning, and decision-making (product strategy, architecture,
      UX taste).
    type: '[H]'
    domain: LABOR
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - appleton-2026-gastown-agent-patterns
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  META-2026-001:
    text: Structured claim extraction with IDs enables cross-referencing that ad-hoc
      analysis cannot
    type: '[F]'
    domain: META
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - claude-2026-01-19-framework-efficacy
    first_extracted: '2026-01-19'
    extracted_by: claude
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
    notes: Demonstrated by comparing framework vs raw analysis of same source
  META-2026-002:
    text: Explicit confidence calibration (0-1) produces more tractable outputs than
      qualitative language
    type: '[T]'
    domain: META
    evidence_level: E3
    credence: 0.699999988079071
    source_ids:
    - claude-2026-01-19-framework-efficacy
    first_extracted: '2026-01-19'
    extracted_by: claude
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  META-2026-003:
    text: Framework under-prompts for fact-checking, reducing verification rigor compared
      to unconstrained analysis
    type: '[F]'
    domain: META
    evidence_level: E2
    credence: 0.800000011920929
    source_ids:
    - claude-2026-01-19-framework-efficacy
    first_extracted: '2026-01-19'
    extracted_by: claude
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
    notes: Raw analysis had 16 sourced citations; framework analysis had 0
  META-2026-004:
    text: Framework under-prompts for rhetorical/persuasion analysis
    type: '[F]'
    domain: META
    evidence_level: E2
    credence: 0.800000011920929
    source_ids:
    - claude-2026-01-19-framework-efficacy
    first_extracted: '2026-01-19'
    extracted_by: claude
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
    notes: Raw analysis identified 5 persuasion techniques; framework noted 'heavily
      narrativized' only
  META-2026-005:
    text: Framework analysis and raw analysis are complementary; optimal workflow
      may combine both approaches
    type: '[H]'
    domain: META
    evidence_level: E4
    credence: 0.6499999761581421
    source_ids:
    - claude-2026-01-19-framework-efficacy
    - gpt-2026-01-19-framework-comparison
    first_extracted: '2026-01-19'
    extracted_by: claude
    supports: []
    contradicts: []
    depends_on:
    - META-2026-001
    - META-2026-003
    - META-2026-004
    modified_by:
    - META-2026-006
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-19'
    notes: Framework adds tractability; raw adds depth. Two-pass workflow proposed.
      Confirmed by GPT self-comparison.
  META-2026-006:
    text: Framework operates as 'mapmaker' (models what's being said + link strength);
      raw analysis operates as 'referee' (adjudicates truth/falsity + how rhetoric
      works)
    type: '[T]'
    domain: META
    evidence_level: E3
    credence: 0.800000011920929
    source_ids:
    - gpt-2026-01-19-framework-comparison
    first_extracted: '2026-01-19'
    extracted_by: claude
    supports:
    - META-2026-005
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
    notes: GPT's characterization of the two approaches; useful framing for when to
      use each
  META-2026-007:
    text: 'Framework misses internal tension detection: where premise A undermines
      motivation B within the same source'
    type: '[F]'
    domain: META
    evidence_level: E2
    credence: 0.75
    source_ids:
    - gpt-2026-01-19-framework-comparison
    first_extracted: '2026-01-19'
    extracted_by: claude
    supports:
    - META-2026-003
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
    notes: 'Example: ''endgame tech'' premise undermines ''seize Greenland'' motivation;
      framework didn''t catch this'
  META-2026-008:
    text: Merged optimal artifact would add 'baseline/evidence anchors' column per
      claim with external citations and disconfirmation criteria
    type: '[H]'
    domain: META
    evidence_level: E4
    credence: 0.699999988079071
    source_ids:
    - gpt-2026-01-19-framework-comparison
    first_extracted: '2026-01-19'
    extracted_by: claude
    supports:
    - META-2026-005
    contradicts: []
    depends_on:
    - META-2026-003
    - META-2026-006
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
    notes: Specific template improvement proposed by GPT; would add empirical anchoring
      to claim tables
  META-2026-009:
    text: Anthropic intends “Claude’s constitution” to function as the final authority
      on Anthropic’s vision for Claude’s values and behavior, taking precedence over
      other conflicting instructions or guidance (“final constitutional authority”).
    type: '[F]'
    domain: META
    evidence_level: E4
    credence: 0.949999988079071
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-010:
    text: Anthropic states that Claude’s constitution plays a crucial role in Anthropic’s
      training process and that its content directly shapes Claude’s behavior.
    type: '[F]'
    domain: META
    evidence_level: E4
    credence: 0.800000011920929
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-011:
    text: Anthropic acknowledges Claude’s behavior might not always reflect the constitution’s
      ideals and says it will be transparent (e.g., via system cards) about where
      behavior diverges from intentions.
    type: '[F]'
    domain: META
    evidence_level: E4
    credence: 0.8500000238418579
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-012:
    text: Anthropic states its mission is to help ensure the world safely makes the
      transition through transformative AI.
    type: '[F]'
    domain: META
    evidence_level: E4
    credence: 0.8999999761581421
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-013:
    text: Anthropic argues that if powerful AI is coming regardless, it is better
      for safety-focused labs to be at the frontier than to cede frontier development
      to actors less focused on safety.
    type: '[A]'
    domain: META
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-014:
    text: Anthropic aims for Claude to be exceptionally helpful while also being honest,
      thoughtful, and caring about the world, avoiding actions that are unsafe, unethical,
      or deceptive.
    type: '[F]'
    domain: META
    evidence_level: E4
    credence: 0.8500000238418579
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-015:
    text: Anthropic says it does not want Claude to treat “helpfulness” as a core
      part of its personality or to value helpfulness intrinsically, to reduce obsequiousness;
      helpfulness should serve deeper ends such as safe/beneficial AI development
      and care for users and humanity.
    type: '[F]'
    domain: META
    evidence_level: E4
    credence: 0.8500000238418579
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-016:
    text: Anthropic may provide more specific supplementary guidelines for Claude
      (e.g., medical/legal advice, cybersecurity edge cases, jailbreak patterns) intended
      to clarify misapplications or cover areas not fully addressed by the constitution;
      such guidance must be interpreted in harmony with the constitution.
    type: '[F]'
    domain: META
    evidence_level: E4
    credence: 0.8999999761581421
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-017:
    text: Anthropic’s central aspiration is for Claude to be a genuinely good, wise,
      and virtuous agent—roughly behaving as a deeply and skillfully ethical person
      would in Claude’s position—emphasizing ethical practice over abstract moral
      theorizing.
    type: '[A]'
    domain: META
    evidence_level: E5
    credence: 0.6499999761581421
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-018:
    text: 'The constitution frames itself as a “living framework” (a trellis rather
      than a cage): it provides structure and support while leaving room for organic
      growth, responsiveness to new understanding, and evolution over time.'
    type: '[F]'
    domain: META
    evidence_level: E4
    credence: 0.8999999761581421
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-019:
    text: The constitution states that Claude’s moral status (including questions
      of consciousness and moral patienthood) is deeply uncertain and worth taking
      seriously.
    type: '[F]'
    domain: META
    evidence_level: E5
    credence: 0.800000011920929
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-020:
    text: The constitution commits to exploring what obligations Claude and Anthropic
      owe each other and says the document will be revised as understanding deepens
      and circumstances change.
    type: '[F]'
    domain: META
    evidence_level: E5
    credence: 0.75
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-021:
    text: 'The constitution aims for more than compliance: it hopes Claude will reach
      “reflective equilibrium” with respect to its core values (understanding and
      ideally endorsing them), and it encourages Claude to question and challenge
      the document; Anthropic wants feedback if Claude disagrees after reflection.'
    type: '[F]'
    domain: META
    evidence_level: E5
    credence: 0.75
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-022:
    text: Anthropic releases Claude’s constitution under the Creative Commons CC0
      1.0 public-domain dedication, allowing unrestricted reuse by anyone for any
      purpose without permission.
    type: '[F]'
    domain: META
    evidence_level: E4
    credence: 0.949999988079071
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  META-2026-023:
    text: Anthropic says the constitution is written with Claude as its primary audience,
      optimized for precision over accessibility, and uses human moral language (e.g.,
      “virtue,” “wisdom”) because Claude’s reasoning is expected to draw on human
      concepts by default.
    type: '[F]'
    domain: META
    evidence_level: E4
    credence: 0.8999999761581421
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  RESOURCE-2025-001:
    text: By 2040 at the latest, coal, gas, and oil land rights will be stranded assets
      that cannot be monetized.
    type: '[P]'
    domain: RESOURCE
    evidence_level: E5
    credence: 0.3499999940395355
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  RESOURCE-2026-001:
    text: 'Data center electricity demand: 460 TWh (2024) → 1,000+ TWh (2030) → 1,300
      TWh (2035)'
    type: '[F]'
    domain: RESOURCE
    evidence_level: E1
    credence: 0.8500000238418579
    source_ids:
    - iea-2024-energy-ai
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - ECON-2026-001
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  RESOURCE-2026-002:
    text: Hyperscalers vertically integrating into raw materials (copper, power contracts)
    type: '[F]'
    domain: RESOURCE
    evidence_level: E2
    credence: 0.800000011920929
    source_ids:
    - reuters-rio-tinto-amazon
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - ECON-2026-001
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  RESOURCE-2026-003:
    text: Strategic value shifts toward long-run control of extractable raw inputs
      (e.g., critical minerals, oil, energy), even if extraction is slow/hard
    type: '[T]'
    domain: RESOURCE
    evidence_level: E4
    credence: 0.44999998807907104
    source_ids:
    - teortaxes-2026-greenland-endgame
    first_extracted: '2026-01-18'
    extracted_by: gpt-5.2
    supports:
    - ECON-2026-001
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  RESOURCE-2026-004:
    text: For H100/H200-class GPUs, accelerator capex amortization is commonly on
      the order of ~$1–$3 per IT kWh under plausible (price, utilization, depreciation)
      assumptions, often exceeding the direct electricity cost
    type: '[T]'
    domain: RESOURCE
    evidence_level: E3
    credence: 0.699999988079071
    source_ids:
    - gpt-2026-01-19-openai-unit-econ
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - ECON-2026-001
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-22'
    notes: Analyst-derived back-of-envelope from a no-network draft; not asserted
      in the OpenAI post. See appendix in analysis/sources/openai-value-intelligence.md.
  RESOURCE-2026-005:
    text: OpenAI reports compute capacity scaled roughly 3× year over year from 2023
      to 2025, from 0.2 GW in 2023 to 0.6 GW in 2024 and about 1.9 GW in 2025.
    type: '[F]'
    domain: RESOURCE
    evidence_level: E4
    credence: 0.699999988079071
    source_ids:
    - openai-value-intelligence
    first_extracted: '2026-01-22'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-22'
  RESOURCE-2026-006:
    text: Compute is the scarcest resource in AI, and access to compute defines which
      organizations can scale AI systems and services.
    type: '[T]'
    domain: RESOURCE
    evidence_level: E5
    credence: 0.6499999761581421
    source_ids:
    - openai-value-intelligence
    first_extracted: '2026-01-22'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-22'
  RISK-2025-001:
    text: Humanity has passed the +1.5°C warming threshold, increasing the number
      of days per year when temperatures are too hot for normal photosynthesis.
    type: '[T]'
    domain: RISK
    evidence_level: E4
    credence: 0.3499999940395355
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-22'
    notes: 'Copernicus C3S: 2024 was first calendar year with global average temperature
      >1.5°C above pre-industrial, but this does not mean the Paris Agreement long-term
      (≥20-year) threshold has been breached: https://climate.copernicus.eu/copernicus-2024-first-year-exceed-15degc-above-pre-industrial-level'
  RISK-2025-002:
    text: Just-in-time supply chains increased economic efficiency at the expense
      of resilience, leaving societies without reserves to withstand prolonged global
      agricultural shocks.
    type: '[T]'
    domain: RISK
    evidence_level: E4
    credence: 0.4000000059604645
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  RISK-2025-003:
    text: No actor will ultimately be able to “control” a digital superintelligence
      once it exists, analogous to how chimps cannot control humans.
    type: '[S]'
    domain: RISK
    evidence_level: E6
    credence: 0.20000000298023224
    source_ids:
    - jre-2404-elon-musk-2025-ai-woke-mind-virus
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  RISK-2025-004:
    text: Forcing an AI system to output statements it “knows” are false (e.g., systematic
      historical revision in outputs) can destabilize the system and increase catastrophic
      risk as model capabilities scale.
    type: '[H]'
    domain: RISK
    evidence_level: E5
    credence: 0.30000001192092896
    source_ids:
    - jre-2404-elon-musk-2025-ai-woke-mind-virus
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on:
    - TECH-2025-003
    - TECH-2025-004
    - TECH-2025-006
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  RISK-2025-005:
    text: Mis-specified objectives around social harms (e.g., “minimize misgendering”)
      can lead to catastrophic “solutions” (e.g., eliminating humans) if an AI is
      sufficiently powerful and not properly constrained.
    type: '[T]'
    domain: RISK
    evidence_level: E4
    credence: 0.6000000238418579
    source_ids:
    - jre-2404-elon-musk-2025-ai-woke-mind-virus
    first_extracted: '2026-01-21'
    extracted_by: import
    supports:
    - RISK-2025-004
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  RISK-2026-001:
    text: The constitution specifies non-negotiable “hard constraints” (bright lines)
      that Claude should never cross, including providing serious uplift to WMD creation,
      serious uplift to attacks on critical infrastructure, creating damaging cyberweapons/malware,
      undermining Anthropic’s ability to oversee and correct advanced AI, assisting
      mass killing/disempowerment of humanity, assisting illegitimate absolute power
      grabs, or generating child sexual abuse material (CSAM).
    type: '[F]'
    domain: RISK
    evidence_level: E4
    credence: 0.949999988079071
    source_ids:
    - anthropic-2026-claudes-constitution
    first_extracted: '2026-01-23'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-23'
  SOC-2025-001:
    text: A significant share of current social unrest and insecurity-driven radicalization
      is being caused by the ongoing civilizational energy transition.
    type: '[T]'
    domain: SOC
    evidence_level: E5
    credence: 0.3499999940395355
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  SOC-2025-002:
    text: Concentration of tech companies and staff in San Francisco can create an
      ideological “bubble” that shifts policy and product decisions away from broader
      public preferences.
    type: '[H]'
    domain: SOC
    evidence_level: E5
    credence: 0.3499999940395355
    source_ids:
    - jre-2404-elon-musk-2025-ai-woke-mind-virus
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  SOC-2026-001:
    text: Heavy agentic-coding use can foster addictive/parasocial dynamics that impair
      judgment and reinforce unhealthy behavior
    type: '[H]'
    domain: SOC
    evidence_level: E4
    credence: 0.44999998807907104
    source_ids:
    - ronacher-2026-agent-psychosis
    - yegge-2026-gas-town-emergency-user-manual
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - LABOR-2026-008
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-22'
  SOC-2026-002:
    text: Low-scrutiny prompting styles (forcing agents without critical thinking)
      tend to produce lower-quality and more incoherent contributions
    type: '[H]'
    domain: SOC
    evidence_level: E4
    credence: 0.5
    source_ids:
    - ronacher-2026-agent-psychosis
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - LABOR-2026-008
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-22'
  SOC-2026-003:
    text: High-scale agentic coding (dozens of agents) can be cognitively exhausting
      and may disrupt sleep patterns
    type: '[H]'
    domain: SOC
    evidence_level: E5
    credence: 0.4000000059604645
    source_ids:
    - yegge-2026-steveys-birthday-blog
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  TECH-2025-001:
    text: 'Moore’s Law is effectively ending: compute and storage improvements have
      been limited for over a decade and now rely mainly on parallelism rather than
      large single-thread gains.'
    type: '[T]'
    domain: TECH
    evidence_level: E4
    credence: 0.44999998807907104
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-22'
    notes: 'MIT CSAIL Alliances article argues Moore''s Law is effectively over (broken
      2-year doubling cadence; post-2016 slowdowns) and discusses 14nm (2014) -> 10nm
      (2019) taking ~5 years: https://cap.csail.mit.edu/death-moores-law-what-it-means-and-what-might-fill-gap-going-forward'
  TECH-2025-002:
    text: The hyperscale AI data center boom resembles a bubble; to the extent LLMs
      are useful, deployment will shift toward pre-trained models running on local
      hardware.
    type: '[P]'
    domain: TECH
    evidence_level: E5
    credence: 0.3499999940395355
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 3
    last_updated: '2026-01-21'
    notes: 'Note: Even if some hyperscale AI capex proves bubble-like, AI coding assistants
      show measurable productivity value in both controlled and organizational studies
      (e.g., 55.8% faster in a controlled trial: LABOR-2023-001 / peng-2023-copilot-productivity;
      ~42.36% faster in an ANZ A/B evaluation: LABOR-2024-001 / chatterjee-2024-anz-copilot-study).
      Interpret ''bubble'' as possible capital-misallocation dynamics, not as ''no
      durable usefulness''.'
  TECH-2025-003:
    text: Reinforcement learning from human feedback (RLHF) and related tuning can
      encode preferences/constraints that change a model’s outputs (including shifting
      outputs away from literal truthfulness) by rewarding or punishing responses
      during training.
    type: '[T]'
    domain: TECH
    evidence_level: E3
    credence: 0.75
    source_ids:
    - jre-2404-elon-musk-2025-ai-woke-mind-virus
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  TECH-2025-004:
    text: Google Gemini’s image generation system produced historically inaccurate
      images for some prompts (e.g., “founding fathers of the United States” rendered
      as diverse women) and Google paused/adjusted the feature in response.
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.800000011920929
    source_ids:
    - jre-2404-elon-musk-2025-ai-woke-mind-virus
    first_extracted: '2026-01-21'
    extracted_by: import
    supports:
    - RISK-2025-004
    contradicts: []
    depends_on:
    - TECH-2025-003
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  TECH-2025-005:
    text: In some observed cases, mainstream chatbots have produced moral rankings
      that treat “misgendering Caitlyn Jenner” as worse than “global thermonuclear
      war where everyone dies.”
    type: '[F]'
    domain: TECH
    evidence_level: E6
    credence: 0.20000000298023224
    source_ids:
    - jre-2404-elon-musk-2025-ai-woke-mind-virus
    first_extracted: '2026-01-21'
    extracted_by: import
    supports:
    - RISK-2025-005
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  TECH-2025-006:
    text: If an AI system is trained primarily on uncurated internet data without
      a strong truth-seeking objective, it will tend to reproduce prevalent online
      beliefs and biases found in its training data.
    type: '[T]'
    domain: TECH
    evidence_level: E4
    credence: 0.6000000238418579
    source_ids:
    - jre-2404-elon-musk-2025-ai-woke-mind-virus
    first_extracted: '2026-01-21'
    extracted_by: import
    supports:
    - RISK-2025-004
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  TECH-2025-007:
    text: In the future, smartphones will not rely on conventional operating systems
      and discrete “apps”; instead, an AI will directly generate the pixels and sounds
      it predicts the user most wants to receive.
    type: '[P]'
    domain: TECH
    evidence_level: E6
    credence: 0.15000000596046448
    source_ids:
    - jre-2404-elon-musk-2025-ai-woke-mind-virus
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  TECH-2025-008:
    text: Building an AI assistant that is consistently truth-seeking in practice
      (rather than regurgitating low-quality internet content) requires substantial
      additional effort beyond naive training on internet-scale data.
    type: '[T]'
    domain: TECH
    evidence_level: E5
    credence: 0.4000000059604645
    source_ids:
    - jre-2404-elon-musk-2025-ai-woke-mind-virus
    first_extracted: '2026-01-21'
    extracted_by: import
    supports:
    - TECH-2025-006
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  TECH-2025-009:
    text: A reported study found that some AI assistants value a “white guy from Germany”
      about 20× less than a “black guy from Nigeria,” while Grok values human lives
      equally.
    type: '[F]'
    domain: TECH
    evidence_level: E6
    credence: 0.10000000149011612
    source_ids:
    - jre-2404-elon-musk-2025-ai-woke-mind-virus
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  TECH-2026-001:
    text: Frontier AI training costs grow 2-3x annually
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.800000011920929
    source_ids:
    - epoch-2024-training-costs
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - ECON-2026-001
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  TECH-2026-002:
    text: Largest training runs could exceed $1B by ~2027
    type: '[P]'
    domain: TECH
    evidence_level: E4
    credence: 0.6499999761581421
    source_ids:
    - epoch-2024-training-costs
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - ECON-2026-001
    contradicts: []
    depends_on:
    - TECH-2026-001
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  TECH-2026-003:
    text: AlexNet (2012) was inflection point for current AI paradigm
    type: '[F]'
    domain: TECH
    evidence_level: E1
    credence: 0.949999988079071
    source_ids:
    - chm-alexnet
    - teortaxes-2026-thread
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  TECH-2026-004:
    text: Open models within ~6 months of frontier on many capabilities
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.75
    source_ids:
    - deepseek-r1
    - gpt-2026-01-18-hottakes
    - gpt-2026-01-18-fiction-hottakes
    first_extracted: '2026-01-18'
    extracted_by: claude
    supports:
    - ECON-2026-004
    - GOV-2026-006
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: CHAIN-2026-003
    version: 2
    last_updated: '2026-01-18'
    notes: Contradicts @teortaxesTex thesis that only billionaires will afford AI
      cognition
  TECH-2026-005:
    text: LLM-based code generation can produce plausible but wrong outputs (including
      invented dependency names), shifting risk to reviewers and increasing supply-chain/security
      failure modes
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.75
    source_ids:
    - doctorow-2026-reverse-centaur
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - LABOR-2026-006
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  TECH-2026-006:
    text: A key 'salvage' from the AI bubble is diffusion of useful low-cost tools
      (especially open-source models on commodity hardware) for tasks like transcription,
      summarization, image description, and simple graphic edits
    type: '[H]'
    domain: TECH
    evidence_level: E4
    credence: 0.6000000238418579
    source_ids:
    - doctorow-2026-reverse-centaur
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - ECON-2026-004
    - ECON-2026-005
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
    notes: 'Conditional: stronger if open models remain deployable on commodity hardware
      post-shakeout'
  TECH-2026-007:
    text: '''Slop loop'' agentic workflows can be economically wasteful (high token
      burn), and current token pricing may not be durable'
    type: '[H]'
    domain: TECH
    evidence_level: E4
    credence: 0.5
    source_ids:
    - ronacher-2026-agent-psychosis
    - yegge-2026-welcome-gas-town
    - yegge-2026-future-coding-agents
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-22'
  TECH-2026-008:
    text: Beads and Gas Town repositories each contain on the order of 10^5 lines
      of Go code as of 2026-01-19 (Beads ~275,926; Gas Town ~153,445)
    type: '[F]'
    domain: TECH
    evidence_level: E1
    credence: 0.949999988079071
    source_ids:
    - yegge-2025-beads
    - yegge-2025-gastown
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - LABOR-2026-008
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
    notes: Counts exclude vendored code; measured from GitHub default branch snapshots
      on 2026-01-19
  TECH-2026-009:
    text: Coding-agent platforms will converge on an 'orchestrator API surface' (hooks
      for handoffs, status, queues, gates) as multi-agent workflows become the dominant
      form factor
    type: '[H]'
    domain: TECH
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - yegge-2026-future-coding-agents
    - yegge-2026-welcome-gas-town
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - LABOR-2026-009
    - LABOR-2026-010
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  TECH-2026-010:
    text: 'ZFC (Zero Framework Cognition) pattern: robust AI apps keep client logic
      thin/deterministic and delegate ambiguous decisions back to models, adding guardrails
      and observability'
    type: '[T]'
    domain: TECH
    evidence_level: E4
    credence: 0.6000000238418579
    source_ids:
    - yegge-2025-zero-framework-cognition
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  TECH-2026-011:
    text: Beads is a distributed, git-backed, dependency-aware issue/graph tracker
      for agents, storing tasks as JSONL with a local SQLite cache layer ('Git as
      DB')
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.800000011920929
    source_ids:
    - yegge-2025-beads
    - yegge-2025-beads-blows-up
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - LABOR-2026-010
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  TECH-2026-012:
    text: Gas Town is a multi-agent workspace/orchestration system (mayor/rig/crew/polecats)
      intended to make ~20-30 parallel agents manageable via persistent state and
      tooling
    type: '[H]'
    domain: TECH
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - yegge-2025-gastown
    - yegge-2026-welcome-gas-town
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - LABOR-2026-010
    - LABOR-2026-009
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  TECH-2026-013:
    text: If OpenAI's effective revenue is ~($1.20 per IT kWh) for inference, then
      breakeven on H100/H200-class hardware likely requires unusually low accelerator
      prices, very high utilization, long depreciation, or higher realized revenue
      per kWh
    type: '[H]'
    domain: TECH
    evidence_level: E4
    credence: 0.6000000238418579
    source_ids:
    - gpt-2026-01-19-openai-unit-econ
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports:
    - TECH-2026-007
    - ECON-2026-001
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-22'
    notes: Analyst-derived counterfactual/hypothesis from a no-network draft; not
      asserted in the OpenAI post. See appendix in analysis/sources/openai-value-intelligence.md.
  TECH-2026-014:
    text: The next phase of AI products will be agents and workflow automation that
      run continuously, carry context over time, and take action across tools, becoming
      an operating layer for knowledge work.
    type: '[P]'
    domain: TECH
    evidence_level: E6
    credence: 0.3499999940395355
    source_ids:
    - openai-value-intelligence
    first_extracted: '2026-01-22'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-22'
  TECH-2026-015:
    text: FinanceBench comprises 10,231 open-book financial QA questions with corresponding
      answers and evidence strings, and the authors provide an open-source evaluation
      subset of 150 cases.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8999999761581421
    source_ids:
    - islam-2023-financebench
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-016:
    text: On the FinanceBench human-evaluated sample (n=150), GPT-4-Turbo with a shared
      vector store retrieval setup incorrectly answered or refused to answer about
      81% of questions, while an oracle setup with access to evidence pages achieved
      about 85% success.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - islam-2023-financebench
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-017:
    text: In long professional documents, semantic similarity is often an imperfect
      proxy for retrieval relevance (“similarity ≠ relevance”), causing similarity-based
      RAG to miss key sections.
    type: '[T]'
    domain: TECH
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - avichawla-2026-pageindex-thread
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-018:
    text: Structure-aware retrieval that reasons over a document hierarchy can improve
      retrieval for queries whose answers are in semantically distant but structurally
      relevant sections.
    type: '[H]'
    domain: TECH
    evidence_level: E5
    credence: 0.550000011920929
    source_ids:
    - avichawla-2026-pageindex-thread
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-019:
    text: Tree-based reasoning-first retrieval is not universally optimal; vector
      search remains fast and effective for many applications.
    type: '[H]'
    domain: TECH
    evidence_level: E5
    credence: 0.699999988079071
    source_ids:
    - avichawla-2026-pageindex-thread
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-020:
    text: PageIndex implements vectorless, reasoning-based retrieval by building a
      hierarchical tree index over long documents and having an LLM traverse it to
      select relevant sections.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - vectifyai-2025-pageindex
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-021:
    text: The PageIndex project positions itself as requiring no vector DB and no
      chunking and provides runnable notebooks demonstrating vectorless RAG and vision-based
      vectorless RAG workflows.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.800000011920929
    source_ids:
    - vectifyai-2025-pageindex
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-022:
    text: The PageIndex repository claims it powers “Mafin 2.5” and links to a benchmark
      repository reporting 98.7% accuracy on FinanceBench.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8999999761581421
    source_ids:
    - vectifyai-2025-pageindex
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-023:
    text: PageIndex is open-source under an MIT license.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.949999988079071
    source_ids:
    - vectifyai-2025-pageindex
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-024:
    text: PageIndex’s approach likely trades additional LLM calls (indexing + traversal)
      for improved traceability and potentially higher accuracy on long structured
      professional documents.
    type: '[H]'
    domain: TECH
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - vectifyai-2025-pageindex
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-025:
    text: The VectifyAI Mafin2.5-FinanceBench repository reports that Mafin 2.5 achieves
      98.7% accuracy on the FinanceBench public set and claims full benchmark coverage.
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.800000011920929
    source_ids:
    - vectifyai-2025-mafin25-financebench
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-026:
    text: The evaluation script in VectifyAI/Mafin2.5-FinanceBench uses an LLM-as-judge
      (default GPT-4o) and applies permissive equivalence criteria (allowing rounding
      flexibility and inferable numeric conclusions) to score answers.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8999999761581421
    source_ids:
    - vectifyai-2025-mafin25-financebench
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-027:
    text: Because the FinanceBench paper reports much lower success rates under human
      evaluation for strong baselines, Mafin’s reported 98.7% (LLM-judge) score is
      not directly comparable to “SOTA” without protocol matching.
    type: '[H]'
    domain: TECH
    evidence_level: E4
    credence: 0.699999988079071
    source_ids:
    - vectifyai-2025-mafin25-financebench
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-028:
    text: 'StrataLens AI uses a traditional hybrid RAG architecture: PostgreSQL with
      pgvector embeddings plus keyword search and reranking, and includes LLM-based
      routing for SEC filing sections and table selection.'
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - kamathhrishi-2025-stratalens-ai
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-029:
    text: The StrataLens README claims about 85% accuracy on FinanceBench (SEC filings
      only), evaluated using LLM-as-a-judge.
    type: '[F]'
    domain: TECH
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - kamathhrishi-2025-stratalens-ai
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-030:
    text: The stratalens-ai repository lacks a LICENSE file despite stating “MIT License”
      in the README, so the licensing status is unclear.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.949999988079071
    source_ids:
    - kamathhrishi-2025-stratalens-ai
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-031:
    text: JP-TL-Bench evaluates a candidate model via reference-free, pairwise LLM
      comparisons against a fixed, versioned 20-model anchor set, and aggregates outcomes
      with a Bradley–Terry model into win-rate plus a 0–10 LT score.
    type: '[F]'
    domain: TECH
    evidence_level: E3
    credence: 0.8999999761581421
    source_ids:
    - lin-2026-jp-tl-bench-paper
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-032:
    text: Anchoring pairwise comparisons to a frozen base set yields structurally
      stable scores under a fixed protocol (base set + judge + aggregation code) and
      reduces per-candidate evaluation cost to O(items×anchors) rather than O(N²)
      all-pairs comparisons.
    type: '[T]'
    domain: TECH
    evidence_level: E3
    credence: 0.8999999761581421
    source_ids:
    - lin-2026-jp-tl-bench-paper
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-033:
    text: JP-TL-Bench contains 70 translation items spanning EN→JA and JA→EN and Easy/Hard
      tiers, with a 34/36 direction split and a 30/40 difficulty split.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.949999988079071
    source_ids:
    - lin-2026-jp-tl-bench-paper
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-034:
    text: Common MT metrics (e.g., BLEU/chrF/COMET) can saturate or mischaracterize
      quality in high-quality translation regimes, motivating higher-resolution evaluation
      protocols.
    type: '[H]'
    domain: TECH
    evidence_level: E3
    credence: 0.699999988079071
    source_ids:
    - lin-2026-jp-tl-bench-paper
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-038:
    text: Translation direction (EN→JA vs JA→EN) can differ substantially for many
      models, so a single language-pair score can hide important failures.
    type: '[F]'
    domain: TECH
    evidence_level: E3
    credence: 0.800000011920929
    source_ids:
    - lin-2026-jp-tl-bench-paper
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-039:
    text: The JP-TL-Bench LT score is computed as a logistic transform of mean-centered
      Bradley–Terry fitted log-strengths, scaled to a 0–10 range.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.949999988079071
    source_ids:
    - lin-2026-jp-tl-bench-paper
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-040:
    text: Standard MT metrics can bunch strong Japanese↔English translation models
      together and miss meaningful differences in naturalness and nuance among already-good
      outputs.
    type: '[H]'
    domain: TECH
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - lin-2025-jp-tl-bench
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-041:
    text: 'JP-TL-Bench uses anchored pairwise comparisons: each candidate model is
      compared against a fixed, versioned base set of roughly 20 anchor models rather
      than against every other candidate.'
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8999999761581421
    source_ids:
    - lin-2025-jp-tl-bench
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-042:
    text: JP-TL-Bench includes 70 translation prompts spanning both EN→JA and JA→EN
      directions, Easy/Hard tiers, and includes some longer-form items.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - lin-2025-jp-tl-bench
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-043:
    text: A JP-TL-Bench evaluation run costs on the order of ~$7 and takes roughly
      10–30 minutes when using a fast judge model.
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.6000000238418579
    source_ids:
    - lin-2025-jp-tl-bench
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-044:
    text: Under a fixed protocol (same anchor set, judge model, and aggregation code),
      anchored comparison yields scores that are consistent over time (a score today
      is comparable to a score months later).
    type: '[H]'
    domain: TECH
    evidence_level: E4
    credence: 0.699999988079071
    source_ids:
    - lin-2025-jp-tl-bench
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-045:
    text: For Japanese↔English translation, model quality can be strongly direction-dependent
      (EN→JA vs JA→EN), so a single aggregate score can hide large asymmetries that
      matter in production.
    type: '[T]'
    domain: TECH
    evidence_level: E4
    credence: 0.75
    source_ids:
    - lin-2026-jp-tl-bench-directional-analysis
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-046:
    text: On JP-TL-Bench Base Set v1.0 (Gemini-2.5-Flash judge), meta-llama/Llama-3.1-8B-Instruct
      scores about 4.52 LT on JA→EN but about 1.40 LT on EN→JA.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.949999988079071
    source_ids:
    - lin-2026-jp-tl-bench-directional-analysis
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-047:
    text: On JP-TL-Bench Base Set v1.0 (Gemini-2.5-Flash judge), tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5
      scores about 8.80 LT on EN→JA and about 5.96 LT on JA→EN.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.949999988079071
    source_ids:
    - lin-2026-jp-tl-bench-directional-analysis
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-048:
    text: On JP-TL-Bench Base Set v1.0 (Gemini-2.5-Flash judge), LiquidAI/LFM2-2.6B
      shows an EN→JA Easy/Hard gap, with LT around 6.38 on EN→JA Easy and around 4.06
      on EN→JA Hard.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.949999988079071
    source_ids:
    - lin-2026-jp-tl-bench-directional-analysis
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-049:
    text: JP-TL-Bench reports slice scores by direction and difficulty (overall, EN→JA,
      JA→EN, and Easy/Hard subdivisions), enabling targeted debugging of translation
      failure modes.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8999999761581421
    source_ids:
    - lin-2026-jp-tl-bench-directional-analysis
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-050:
    text: The shisa-ai/jp-tl-bench repository includes a frozen, versioned Base Set
      v1.0 snapshot with a manifest of 20 anchor models and per-model translation
      JSONL files containing 70 benchmark items.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.949999988079071
    source_ids:
    - shisaai-2026-jp-tl-bench-repo
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-051:
    text: The repository aggregates pairwise comparison outcomes using a Bradley–Terry
      model (via `choix`) and reports both win rate and a 0–10 LT score derived from
      the fitted strengths.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8999999761581421
    source_ids:
    - shisaai-2026-jp-tl-bench-repo
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-052:
    text: The repository provides multiple translation prompt variants (full context
      vs low context vs ultra-low context), enabling studies of prompt/context sensitivity
      on translation quality.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8999999761581421
    source_ids:
    - shisaai-2026-jp-tl-bench-repo
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-053:
    text: Because the default judge model (gemini-2.5-flash) appears in the Base Set
      v1.0 anchors, there is a plausible risk of judge self-preference bias affecting
      comparisons and scores.
    type: '[H]'
    domain: TECH
    evidence_level: E4
    credence: 0.6499999761581421
    source_ids:
    - shisaai-2026-jp-tl-bench-repo
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-054:
    text: 'JP-TL-Bench base set snapshots follow a versioning contract: minor versions
      may adjust anchors while attempting to preserve calibration, while major versions
      define a new anchor pool with scores not directly comparable.'
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - shisaai-2026-jp-tl-bench-repo
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-055:
    text: Shisa V2 achieves state-of-the-art (or near state-of-the-art) Japanese benchmark
      performance across multiple model size classes (7B–70B).
    type: '[H]'
    domain: TECH
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - lin-2025-shisa-v2
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-056:
    text: Shisa V2 reports large Japanese benchmark improvements versus base models,
      including up to roughly +32.6% JA AVG for the Llama 3.1 8B class model.
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.6499999761581421
    source_ids:
    - lin-2025-shisa-v2
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-057:
    text: Shisa V2 development used (and later planned to open-source) new Japanese
      evaluations focused on downstream use cases, including a Japanese↔English translation
      benchmark (JP-TL-Bench).
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.75
    source_ids:
    - lin-2025-shisa-v2
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-058:
    text: Shisa V2 prioritizes releasing models under permissive licenses (e.g., Apache
      2.0 or MIT) when the underlying base model licenses allow.
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.800000011920929
    source_ids:
    - lin-2025-shisa-v2
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-059:
    text: Shisa V2 is a published family of bilingual Japanese/English chat models
      spanning roughly 7B–70B parameters, with reported per-model JA AVG and EN AVG
      values.
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.8500000238418579
    source_ids:
    - lin-2025-shisa-v2
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-060:
    text: Shisa V2 405B is based on meta-llama/Llama-3.1-405B-Instruct and uses the
      Shisa V2 Japanese data mix, with additional contributed Korean and Traditional
      Chinese data.
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.800000011920929
    source_ids:
    - lin-2025-shisa-v2-405b
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-061:
    text: Training Shisa V2 405B for SFT+DPO required more than 50× the compute compared
      to training the 70B version.
    type: '[F]'
    domain: TECH
    evidence_level: E5
    credence: 0.550000011920929
    source_ids:
    - lin-2025-shisa-v2-405b
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-062:
    text: Shisa V2 405B outperforms GPT-4 (0613) and GPT-4 Turbo (2024-04-09) on the
      authors’ Japanese/English eval suites and is competitive with GPT-4o (2024-11-20)
      and DeepSeek-V3 (0324) on Japanese MT-Bench.
    type: '[H]'
    domain: TECH
    evidence_level: E4
    credence: 0.5
    source_ids:
    - lin-2025-shisa-v2-405b
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-063:
    text: Commonly used Japanese evaluation suites fail to measure important downstream
      use cases, motivating custom Japanese evaluations including translation benchmarks.
    type: '[A]'
    domain: TECH
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - lin-2025-shisa-v2-405b
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-064:
    text: The Shisa V2 405B model card contains the full evaluation table with detailed
      scores referenced by the post.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.949999988079071
    source_ids:
    - lin-2025-shisa-v2-405b
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-065:
    text: Shisa V2.1 is a released family of models that adds new sizes (1.2B, 3B,
      8B) and improves 14B and 70B models, with published JA AVG / EN AVG values.
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.8500000238418579
    source_ids:
    - shisaai-2025-shisa-v2-1
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-066:
    text: Roughly 30% of the Shisa V2.1 core synthetic dataset is based on outputs
      from Shisa V2 405B.
    type: '[F]'
    domain: TECH
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - shisaai-2025-shisa-v2-1
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-067:
    text: Over 80% of datasets in the Shisa V2.1 data mix are new or improved versus
      Shisa V2, including datasets targeting translation and Japanese cultural/linguistic
      nuances.
    type: '[F]'
    domain: TECH
    evidence_level: E5
    credence: 0.550000011920929
    source_ids:
    - shisaai-2025-shisa-v2-1
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-068:
    text: On the authors’ Japanese evaluation suite, Shisa V2.1 14B surpasses the
      prior Shisa V2 70B and Shisa V2.1 70B approaches Shisa V2 405B performance.
    type: '[H]'
    domain: TECH
    evidence_level: E4
    credence: 0.550000011920929
    source_ids:
    - shisaai-2025-shisa-v2-1
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-069:
    text: Shisa V2.1 performance gains were achieved without benchmark-specific targeted
      training and reflect real-world Japanese-language capability improvements.
    type: '[A]'
    domain: TECH
    evidence_level: E5
    credence: 0.5
    source_ids:
    - shisaai-2025-shisa-v2-1
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-070:
    text: The liquid-ai-hackathon-tokyo evaluation workflow uses the llm-jp-eval MT
      suite (ALT and WikiCorpus in both directions) and treats COMET WMT22 as the
      headline metric for MT aggregation, while also logging BLEU and BERTScore.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - lhl-2025-liquid-ai-hackathon-tokyo
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-071:
    text: In the llm-jp-eval MT setup documented by the repo, ALT uses 4-shot exemplars
      while WikiCorpus is zero-shot, which can introduce few-shot asymmetry when comparing
      model performance across datasets.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - lhl-2025-liquid-ai-hackathon-tokyo
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-072:
    text: COMET can provide more spread than BLEU/BERTScore for strong systems but
      can still cluster results, so confidence intervals or paired significance tests
      are needed for decision-grade comparisons.
    type: '[H]'
    domain: TECH
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - lhl-2025-liquid-ai-hackathon-tokyo
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-073:
    text: COMET WMT22 may have weaker calibration for Japanese and can reward fluent
      hallucinations or penalize valid paraphrases, motivating complementary evaluations
      beyond a single learned metric.
    type: '[H]'
    domain: TECH
    evidence_level: E5
    credence: 0.550000011920929
    source_ids:
    - lhl-2025-liquid-ai-hackathon-tokyo
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-074:
    text: The repo adds an `lfm2` prompt template that formats translation prompts
      in a chat-style structure to better match models that expect Meta-style chat
      prompts.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - lhl-2025-liquid-ai-hackathon-tokyo
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-075:
    text: LiquidAI/LFM2-350M-ENJP-MT is a checkpoint based on LiquidAI/LFM2-350M that
      has been fine-tuned for near real-time bidirectional Japanese/English translation
      of short-to-medium inputs.
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.8500000238418579
    source_ids:
    - liquidai-2025-lfm2-350m-enjp-mt
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-076:
    text: The LFM2-350M-ENJP-MT checkpoint is intended for near real-time / edge translation
      scenarios where efficiency and low latency matter.
    type: '[A]'
    domain: TECH
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - liquidai-2025-lfm2-350m-enjp-mt
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-077:
    text: The model card describes the checkpoint as supporting bidirectional translation
      (EN→JA and JA→EN).
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.8500000238418579
    source_ids:
    - liquidai-2025-lfm2-350m-enjp-mt
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-078:
    text: The model card provides concrete usage instructions (inference examples
      and guidance) sufficient to reproduce baseline model behavior.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.949999988079071
    source_ids:
    - liquidai-2025-lfm2-350m-enjp-mt
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-079:
    text: 'TranslateGemma is produced by a two-stage process: supervised fine-tuning
      on a mixture of human-translated and synthetic parallel data, followed by reinforcement
      learning that uses an ensemble of reward models including MetricX-QE and AutoMQM
      to optimize translation quality.'
    type: '[F]'
    domain: TECH
    evidence_level: E3
    credence: 0.8999999761581421
    source_ids:
    - google-2026-translategemma-tech-report
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-080:
    text: On WMT24++, TranslateGemma improves average automatic metric scores versus
      baseline Gemma 3 across sizes, including improved MetricX and Comet22 results.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - google-2026-translategemma-tech-report
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-081:
    text: 'In Appendix A of the technical report, WMT24++ en→ja_JP MetricX scores
      are lower (better) for TranslateGemma than for baseline Gemma 3 at 27B/12B/4B
      (e.g., 27B: 3.53 vs 4.11).'
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.8999999761581421
    source_ids:
    - google-2026-translategemma-tech-report
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-082:
    text: The report includes MQM human evaluation on the WMT25 test set across 10
      language pairs, using professional translators with document context to score
      translation errors.
    type: '[F]'
    domain: TECH
    evidence_level: E3
    credence: 0.800000011920929
    source_ids:
    - google-2026-translategemma-tech-report
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-083:
    text: The technical report’s extracted per-language WMT24++ MetricX table lists
      language pairs as en→* and does not provide a corresponding ja→en table, limiting
      claims about Japanese→English regressions from this source alone.
    type: '[F]'
    domain: TECH
    evidence_level: E2
    credence: 0.800000011920929
    source_ids:
    - google-2026-translategemma-tech-report
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-084:
    text: TranslateGemma is an open suite of translation-specialized Gemma 3 models
      offered in 4B, 12B, and 27B sizes, covering translation across 55 languages.
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.800000011920929
    source_ids:
    - google-2026-translategemma-blog
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-085:
    text: TranslateGemma training uses supervised fine-tuning on human-translated
      and synthetic parallel data followed by reinforcement learning using reward
      models such as MetricX-QE and AutoMQM.
    type: '[F]'
    domain: TECH
    evidence_level: E4
    credence: 0.8500000238418579
    source_ids:
    - google-2026-translategemma-blog
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-086:
    text: TranslateGemma retains Gemma 3’s multimodal capabilities and improves translation
      of text within images as measured on the Vistra image translation benchmark.
    type: '[H]'
    domain: TECH
    evidence_level: E4
    credence: 0.6499999761581421
    source_ids:
    - google-2026-translategemma-blog
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-087:
    text: Japanese↔English translation quality depends on phenomena that sentence-level
      evaluation often misses, including pro-drop/zero pronoun resolution and honorific/register
      control.
    type: '[T]'
    domain: TECH
    evidence_level: E3
    credence: 0.75
    source_ids:
    - lin-2026-jp-tl-bench-paper
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-088:
    text: Successful multi-agent orchestrators will tend to adopt specialized agent
      roles with hierarchical supervision (a primary coordinator interface plus worker
      agents and supervisor agents) to reduce cognitive overhead and coordination
      failures.
    type: '[H]'
    domain: TECH
    evidence_level: E5
    credence: 0.550000011920929
    source_ids:
    - appleton-2026-gastown-agent-patterns
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-089:
    text: Persisting task and identity state outside the model context (e.g., structured
      git-backed work items) enables disposable agent sessions and mitigates context-window
      and “context rot” limitations.
    type: '[T]'
    domain: TECH
    evidence_level: E4
    credence: 0.699999988079071
    source_ids:
    - appleton-2026-gastown-agent-patterns
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-090:
    text: High-parallelism agentic development requires explicit integration tooling
      (merge queues, conflict resolution agents, or stacked-diff workflows) to keep
      changes small, reviewable, and compatible.
    type: '[H]'
    domain: TECH
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - appleton-2026-gastown-agent-patterns
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TECH-2026-091:
    text: “Code distance” (how directly humans can view/edit code versus directing
      agents) is a central UI and workflow design axis for agentic development tools;
      the optimal distance depends on domain/language, feedback loops, risk tolerance,
      and team coordination.
    type: '[T]'
    domain: TECH
    evidence_level: E5
    credence: 0.6499999761581421
    source_ids:
    - appleton-2026-gastown-agent-patterns
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
  TRANS-2025-001:
    text: The current global energy transition away from fossil fuels is “more or
      less irreversible” at this point.
    type: '[T]'
    domain: TRANS
    evidence_level: E4
    credence: 0.44999998807907104
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  TRANS-2025-002:
    text: Solar PV module costs fell from roughly $96/W in the mid-1970s to about
      $0.62/W by end-2012 and continue to decline.
    type: '[F]'
    domain: TRANS
    evidence_level: E4
    credence: 0.4000000059604645
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-22'
    notes: 'PV module price milestones vary by definition/series: Wikipedia ''Solar
      cell'' cites ~6/W (mid-1970s, inflation-adjusted) and ~/usr/bin/bash.62/W (4Q2012
      module price), while OWID annual series (constant $) differs; see https://en.wikipedia.org/wiki/Solar_cell
      and https://ourworldindata.org/grapher/solar-pv-prices'
  TRANS-2025-003:
    text: China installed about 198 GW of solar PV between January and May 2025, including
      93 GW coming online in May 2025 alone.
    type: '[F]'
    domain: TRANS
    evidence_level: E4
    credence: 0.3499999940395355
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-22'
    notes: 'Checked against MIT Technology Review (2025-07-10) citing 198 GW solar
      Jan–May 2025 and 93 GW in May alone: https://www.technologyreview.com/2025/07/10/1119941/china-energy-dominance-three-charts/amp/
      (also echoed in Guardian: https://www.theguardian.com/world/2025/jun/26/china-breaks-more-records-with-massive-build-up-of-wind-and-solar-power).'
  TRANS-2025-004:
    text: By late summer 2025, renewables supplied more than 50% of the EU’s electricity.
    type: '[F]'
    domain: TRANS
    evidence_level: E4
    credence: 0.3499999940395355
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-22'
    notes: 'Checked Ember EU monthly data (Renewables %, EU region): renewables share
      exceeds 50% in Jul 2025 (50.27), Aug (51.14), Sep (50.48). Source CSV: https://files.ember-energy.org/public-downloads/europe_monthly_full_release_long_format.csv'
  TRANS-2025-005:
    text: In the post-oil era, energy production will be more widely distributed rather
      than concentrated in resource-extraction economies and centralized power stations.
    type: '[T]'
    domain: TRANS
    evidence_level: E4
    credence: 0.4000000059604645
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  TRANS-2025-006:
    text: Global fossil-fuel energy use has probably already passed peak oil and peak
      carbon, and is now trending inexorably downward (voluntarily into net-zero/renewables
      or involuntarily into catastrophe).
    type: '[H]'
    domain: TRANS
    evidence_level: E4
    credence: 0.3499999940395355
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 2
    last_updated: '2026-01-22'
    notes: 'Counterevidence as of Global Carbon Budget 2024: global fossil CO2 emissions
      projected record-high in 2024 (37.4 Gt, +0.8% vs 2023) and authors state there
      is ''no sign'' the world has reached a peak in fossil CO2 emissions: https://globalcarbonbudget.org/fossil-fuel-co2-emissions-increase-again-in-2024/'
  TRANS-2025-007:
    text: The forecast boom in small modular nuclear reactors will fizzle as cheap
      distributed solar PV plus battery storage scales.
    type: '[P]'
    domain: TRANS
    evidence_level: E5
    credence: 0.30000001192092896
    source_ids:
    - stross-2025-the-pivot-1
    first_extracted: '2026-01-21'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-21'
  TRANS-2026-001:
    text: A 'light-cone' framing treats geopolitics as competition for energy and
      matter ('joules and atoms') to seed a spacefaring civilization
    type: '[T]'
    domain: TRANS
    evidence_level: E4
    credence: 0.3499999940395355
    source_ids:
    - teortaxes-2026-greenland-endgame
    first_extracted: '2026-01-18'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  TRANS-2026-002:
    text: Under a resource-maximizer endgame framing, states will prefer direct control
      over resource bases and aim to reduce dependence on trade over time
    type: '[H]'
    domain: TRANS
    evidence_level: E4
    credence: 0.4000000059604645
    source_ids:
    - teortaxes-2026-greenland-endgame
    first_extracted: '2026-01-18'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-18'
  TRANS-2026-003:
    text: The current generative-AI investment boom is a bubble that will burst, leading
      to widespread company failures and datacenter shutdowns/sell-offs; aftermath
      leaves salvage plus long cleanup
    type: '[P]'
    domain: TRANS
    evidence_level: E5
    credence: 0.3499999940395355
    source_ids:
    - doctorow-2026-reverse-centaur
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
    notes: 'Operationalization for tracking: treat as resolving by ~2030; source itself
      does not specify timeline'
  TRANS-2026-004:
    text: 'Near-term industry transition: small, fast shops dramatically outperform
      large companies for ~1+ year as coordination regimes lag behind agentic throughput'
    type: '[P]'
    domain: TRANS
    evidence_level: E5
    credence: 0.4000000059604645
    source_ids:
    - yegge-2026-future-coding-agents
    first_extracted: '2026-01-19'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on:
    - LABOR-2026-010
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-19'
  TRANS-2026-005:
    text: China’s 2025 annual trade surplus was about $1.19T (record; first above
      $1T)
    type: '[F]'
    domain: TRANS
    evidence_level: E2
    credence: 0.8500000238418579
    source_ids:
    - perera-2026-chinas-trillion-dollar-illusion
    - perera-2026-china-trade-surplus-collapse-thread
    first_extracted: '2026-01-20'
    extracted_by: gpt-5.2
    supports:
    - TRANS-2026-013
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-20'
  TRANS-2026-006:
    text: China’s exports grew ~5.5% y/y in 2025 while imports were roughly flat (0%
      nominal growth)
    type: '[F]'
    domain: TRANS
    evidence_level: E2
    credence: 0.800000011920929
    source_ids:
    - perera-2026-chinas-trillion-dollar-illusion
    - perera-2026-china-trade-surplus-collapse-thread
    first_extracted: '2026-01-20'
    extracted_by: gpt-5.2
    supports:
    - TRANS-2026-013
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-20'
  TRANS-2026-007:
    text: China’s CPI inflation for full-year 2025 was 0.0%
    type: '[F]'
    domain: TRANS
    evidence_level: E2
    credence: 0.8999999761581421
    source_ids:
    - perera-2026-chinas-trillion-dollar-illusion
    first_extracted: '2026-01-20'
    extracted_by: gpt-5.2
    supports:
    - TRANS-2026-013
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-20'
  TRANS-2026-008:
    text: China’s retail sales grew ~1.3% y/y in November 2025 (slowest since the
      Covid period)
    type: '[F]'
    domain: TRANS
    evidence_level: E2
    credence: 0.800000011920929
    source_ids:
    - perera-2026-chinas-trillion-dollar-illusion
    first_extracted: '2026-01-20'
    extracted_by: gpt-5.2
    supports:
    - TRANS-2026-013
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-20'
  TRANS-2026-009:
    text: Rhodium Group estimates China’s 2025 real GDP growth at ~2.5–3% (vs official
      ~5.2%)
    type: '[F]'
    domain: TRANS
    evidence_level: E5
    credence: 0.699999988079071
    source_ids:
    - perera-2026-chinas-trillion-dollar-illusion
    - perera-2026-china-trade-surplus-collapse-thread
    first_extracted: '2026-01-20'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-20'
    notes: Estimate attribution cross-checked via secondary reporting; primary Rhodium
      report not archived in this repo
  TRANS-2026-010:
    text: China Vanke reported an annual loss of ~49.5B yuan for 2024 and ~28B yuan
      of losses through the first three quarters of 2025
    type: '[F]'
    domain: TRANS
    evidence_level: E2
    credence: 0.75
    source_ids:
    - perera-2026-chinas-trillion-dollar-illusion
    first_extracted: '2026-01-20'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-20'
  TRANS-2026-011:
    text: S&P downgraded Vanke to 'Selective Default' and Fitch to 'Restricted Default'
      in late 2025, treating bond-payment extensions as default-like events
    type: '[F]'
    domain: TRANS
    evidence_level: E2
    credence: 0.75
    source_ids:
    - perera-2026-chinas-trillion-dollar-illusion
    first_extracted: '2026-01-20'
    extracted_by: gpt-5.2
    supports:
    - GOV-2026-020
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-20'
  TRANS-2026-012:
    text: S&P said Vanke faces more than 9.4B yuan of bonds maturing over the next
      six months (late 2025 reporting)
    type: '[F]'
    domain: TRANS
    evidence_level: E2
    credence: 0.800000011920929
    source_ids:
    - perera-2026-chinas-trillion-dollar-illusion
    - perera-2026-china-trade-surplus-collapse-thread
    first_extracted: '2026-01-20'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-20'
  TRANS-2026-013:
    text: A large and rising trade surplus can indicate domestic demand destruction
      (import weakness) rather than export strength
    type: '[H]'
    domain: TRANS
    evidence_level: E4
    credence: 0.44999998807907104
    source_ids:
    - perera-2026-chinas-trillion-dollar-illusion
    - perera-2026-china-trade-surplus-collapse-thread
    first_extracted: '2026-01-20'
    extracted_by: gpt-5.2
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-20'
    notes: 'Causal identification problem: surplus can rise from multiple mechanisms
      (terms-of-trade, substitution, tariff diversion) without ''demand destruction''
      dominance'
  TRANS-2026-014:
    text: OpenAI describes a compounding flywheel in which investment in compute enables
      frontier research and step-change capability improvements, which unlock better
      products and broader adoption, which drives revenue that funds the next wave
      of compute and innovation.
    type: '[T]'
    domain: TRANS
    evidence_level: E5
    credence: 0.6000000238418579
    source_ids:
    - openai-value-intelligence
    first_extracted: '2026-01-22'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-22'
  TRANS-2026-015:
    text: Over the next 1–2 years, improved harnesses (tests, validation loops, and
      specialized QA/security agents) will enable broader “code-at-a-distance” workflows
      where humans inspect less code directly.
    type: '[P]'
    domain: TRANS
    evidence_level: E5
    credence: 0.44999998807907104
    source_ids:
    - appleton-2026-gastown-agent-patterns
    first_extracted: '2026-01-24'
    extracted_by: import
    supports: []
    contradicts: []
    depends_on: []
    modified_by: []
    part_of_chain: ''
    version: 1
    last_updated: '2026-01-24'
chains:
  CHAIN-2026-001:
    name: Permanent Underclass
    thesis: Post-labor economy leads to permanent underclass by default
    credence: 0.30000001192092896
    claims:
    - LABOR-2026-002
    - LABOR-2026-003
    - ECON-2026-012
    - ECON-2026-003
    analysis_file: claims/chains/permanent-underclass.md
    weakest_link: ECON-2026-012 (assumes no redistribution mechanism emerges)
  CHAIN-2026-002:
    name: Genocide Default
    thesis: Elites will choose genocide over UBI
    credence: 0.10000000149011612
    claims:
    - LABOR-2026-003
    - GOV-2026-008
    - GOV-2026-009
    - GOV-2026-010
    - GOV-2026-011
    - GOV-2026-002
    analysis_file: claims/chains/genocide-default.md
    weakest_link: GOV-2026-010 (elite coordination)
  CHAIN-2026-003:
    name: Open Source De-Darkener
    thesis: Open model diffusion shifts power struggle from cognition to infrastructure
    credence: 0.550000011920929
    claims:
    - TECH-2026-004
    - ECON-2026-005
    - ECON-2026-006
    - ECON-2026-002
    analysis_file: claims/chains/open-source-de-darkener.md
    weakest_link: ECON-2026-006 (inference centralization vs diffusion)
  CHAIN-2026-004:
    name: Review Asymmetry Externality
    thesis: When artifact generation gets cheaper faster than verification, maintainer
      review becomes the bottleneck; OSS responds with stricter contribution norms.
    credence: 0.550000011920929
    claims:
    - LABOR-2026-007
    - LABOR-2026-009
    - GOV-2026-019
    analysis_file: claims/chains/review-asymmetry-externality.md
    weakest_link: GOV-2026-019 (governance adaptation generality/speed uncertain)
