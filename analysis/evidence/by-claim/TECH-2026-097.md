# Evidence: TECH-2026-097

> KV-cache memory per sequence scales roughly as KV_bytes = 2 × layers × hidden_dim × bytes_per_param × context_length, so long-context usage for large models can require hundreds of GB per active session—often exceeding a single GPU’s VRAM—and therefore motivates TTLs, tiered cache storage, and KV quantization/paging.

| ID | Direction | Source | Location | Strength | Status |
|----|-----------|--------|----------|----------|--------|
| EVLINK-2026-145 | supports | lhl-2026-frontier-llm-token-unit-economics | Analysis file: analysis/sources/lhl-2026-frontier-llm-token-unit-economics.md (claim tables / Claims to Register YAML); URL: https://github.com/lhl/frontier-llm-token-unit-economics | 0.85 | active |