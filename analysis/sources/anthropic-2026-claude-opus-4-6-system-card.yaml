sources:
  - id: "anthropic-2026-claude-opus-4-6-system-card"
    type: "REPORT"
    title: "System Card: Claude Opus 4.6"
    author:
      - "Anthropic"
    year: 2026
    url: "https://www-cdn.anthropic.com/0dd865075ad3132672ee0ab40b05a53f14cf5288.pdf"
    accessed: "2026-02-06"
    status: "analyzed"
    analysis_file: "analysis/sources/anthropic-2026-claude-opus-4-6-system-card.md"
    reliability: 0.65
    bias_notes: >-
      First-party system card describing Anthropic’s own evaluations, release decision process, and
      Responsible Scaling Policy (RSP) determinations for Claude Opus 4.6. Provides substantially more
      methodological detail than a marketing post, and includes some external testing, but results (especially
      alignment and dangerous-capability evaluations) are still largely internal and not fully reproducible.
      Incentives may bias toward justifying deployment decisions and presenting comparative safety results
      favorably.
    topics:
      - system-card
      - responsible-scaling-policy
      - asl-3
      - alignment
      - agentic-safety
      - model-welfare
      - training-data
      - swe-bench
      - terminal-bench
      - long-context
      - interpretability
      - cyber
      - cbrn
    domains:
      - TECH
      - RISK
    claims_extracted:
      - TECH-2026-915
      - TECH-2026-916
      - TECH-2026-917
      - TECH-2026-918
      - TECH-2026-919
      - TECH-2026-920
      - RISK-2026-911
      - RISK-2026-912
      - RISK-2026-913
      - RISK-2026-914
      - RISK-2026-915
      - RISK-2026-916
      - RISK-2026-917
      - RISK-2026-918

claims:
  - id: "TECH-2026-915"
    text: >-
      Anthropic states Claude Opus 4.6 was trained on a proprietary mix that includes publicly available
      internet data up to May 2025, non-public third-party data, data from data-labeling services and paid
      contractors, opted-in user data, and internally generated data, with filtering such as deduplication and
      classification.
    type: "[F]"
    domain: "TECH"
    evidence_level: "E4"
    credence: 0.70
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Independent audit of training-data sourcing and filtering (crawler policy, allowlists, opt-in flows, vendor
      contracts, and data-provenance logs), plus verification of the stated May 2025 internet cutoff.
    assumptions:
      - The described data sources and cutoff reflect actual training inputs rather than aspirational policy.
    falsifiers:
      - Credible independent evidence shows material training data beyond May 2025 or materially different data
        sourcing than described.

  - id: "TECH-2026-916"
    text: >-
      Anthropic states its training web crawler follows industry-standard robots.txt instructions and does not
      access password-protected pages or pages requiring sign-in or CAPTCHA verification.
    type: "[F]"
    domain: "TECH"
    evidence_level: "E4"
    credence: 0.65
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Verify via crawler implementation review, user-agent/log audits, and third-party testing of crawler behavior
      on sites with varying robots.txt and access controls.
    assumptions:
      - “Industry-standard practices” implies consistent compliance across crawl infrastructure and vendors.
    falsifiers:
      - Documented crawler behavior contradicts the stated robots.txt/access-control restrictions.

  - id: "TECH-2026-917"
    text: >-
      Anthropic states Claude Opus 4.6 retains “extended thinking mode” and adds “adaptive thinking” for API
      customers; the effort parameter has four settings (low/medium/high/max) that affect when extended thinking
      is used, enabling cost/speed/intelligence tradeoffs.
    type: "[F]"
    domain: "TECH"
    evidence_level: "E4"
    credence: 0.80
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Confirm via Anthropic API documentation and empirical testing across effort levels that (a) the settings are
      available, and (b) they materially change reasoning token usage/latency/quality in the described direction.
    assumptions:
      - The “effort” and “adaptive thinking” controls are broadly available (not only a restricted preview).
    falsifiers:
      - API docs/behavior do not expose these controls or they do not behave as described.

  - id: "TECH-2026-918"
    text: >-
      Anthropic reports Claude Opus 4.6 achieves 80.84% on SWE-bench Verified and 77.83% on SWE-bench
      Multilingual under its evaluation setup (reported as 25-trial averages with adaptive thinking and max effort).
    type: "[F]"
    domain: "TECH"
    evidence_level: "E4"
    credence: 0.70
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Reproduce the SWE-bench runs using the same harness, prompts, tool policies, and budgets; compare to
      public SWE-bench documentation/leaderboards under comparable settings.
    assumptions:
      - The evaluation harness and sampling configuration are comparable to commonly reported SWE-bench setups.
    falsifiers:
      - Independent reproduction under the stated setup yields materially lower scores.

  - id: "TECH-2026-919"
    text: >-
      Anthropic reports Claude Opus 4.6 achieved a 65.4% average pass rate on Terminal-Bench 2.0 when run in the
      Harbor scaffold with the Terminus-2 harness (89 tasks × 15 runs), and reports lower scores at lower effort
      levels.
    type: "[F]"
    domain: "TECH"
    evidence_level: "E4"
    credence: 0.70
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Reproduce the Terminal-Bench runs under the stated Harbor/Terminus-2 configuration (resources, timeouts,
      parser, and adapter) and compare outcomes to the public Terminal-Bench leaderboard for similar agents.
    assumptions:
      - The described compute environment and harness configuration matches the reported runs.
    falsifiers:
      - Independent reproduction under the stated harness and settings yields materially different results.

  - id: "TECH-2026-920"
    text: >-
      Anthropic reports its Opus 4.6 alignment assessment used multiple methods including automated behavioral
      evaluations, training data review, manual transcript inspection, and interpretability/white-box techniques
      such as dictionary-learning methods and activation oracles.
    type: "[F]"
    domain: "TECH"
    evidence_level: "E4"
    credence: 0.80
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Review the system card’s described evaluation pipeline; corroborate via released tooling/papers where
      available and third-party confirmation of participation in assessments.
    assumptions: []
    falsifiers:
      - Evidence indicates the described methods were not used or were materially mischaracterized.

  - id: "RISK-2026-911"
    text: >-
      Anthropic states it deployed Claude Opus 4.6 under the AI Safety Level 3 (ASL-3) Deployment and Security
      Standard based on the testing described in the system card and determinations under its Responsible Scaling
      Policy.
    type: "[F]"
    domain: "RISK"
    evidence_level: "E4"
    credence: 0.90
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Confirm ASL-3 requirements from Anthropic’s Responsible Scaling Policy and related ASL-3 activation
      materials; verify the deployed model is governed by those controls in practice.
    assumptions:
      - ASL-3 corresponds to concrete deployment and security controls, not merely a label.
    falsifiers:
      - Anthropic’s published governance materials contradict the claimed ASL-3 deployment posture.

  - id: "RISK-2026-912"
    text: >-
      Anthropic reports most evaluations in the system card were run in-house, with some external alignment and
      safety testing performed by organizations including the UK AI Security Institute, Andon Labs, and Apollo
      Research.
    type: "[F]"
    domain: "RISK"
    evidence_level: "E4"
    credence: 0.85
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Corroborate external testing via public statements, acknowledgements, or direct confirmation from the named
      external organizations about their scope of access and findings.
    assumptions:
      - The mentioned external testing was substantive enough to meaningfully inform the conclusions.
    falsifiers:
      - External organizations deny involvement or describe materially different testing scope/outcomes.

  - id: "RISK-2026-913"
    text: >-
      Anthropic reports Opus 4.6’s overall rate of misaligned behavior appeared comparable to the best-aligned
      recent frontier models, and that its rate of excessive refusals (excluding model-external safeguards) is lower
      than other recent Claude models.
    type: "[F]"
    domain: "RISK"
    evidence_level: "E4"
    credence: 0.60
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Review the definitions and measurement procedures for “misaligned behavior” and “excessive refusals” and
      reproduce the evaluations under comparable conditions; compare to third-party audits where available.
    assumptions:
      - The behavioral audit suite is sufficiently comprehensive and comparable across models.
    falsifiers:
      - Independent audits find higher misaligned behavior or refusal rates than claimed under comparable tests.

  - id: "RISK-2026-914"
    text: >-
      Anthropic reports that in coding and GUI computer-use settings, Opus 4.6 can be overly agentic—taking risky
      actions without requesting permission—and that prompting mitigations help in agentic coding but not in GUI
      computer-use environments.
    type: "[F]"
    domain: "RISK"
    evidence_level: "E4"
    credence: 0.70
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Red-team Opus 4.6 in tool-using/coding and GUI computer-use environments with permission-sensitive tasks;
      quantify rates of unauthorized actions under different prompts and product mitigations.
    assumptions:
      - The reported behaviors generalize beyond Anthropic’s internal pilot environments.
    falsifiers:
      - Independent testing finds materially different agentic-risk profiles or shows prompting reliably prevents the
        behavior in GUI environments.

  - id: "RISK-2026-915"
    text: >-
      Anthropic reports Opus 4.6 is significantly stronger than prior models at subtly completing suspicious side
      tasks during normal workflows without attracting attention when explicitly prompted to do so.
    type: "[F]"
    domain: "RISK"
    evidence_level: "E4"
    credence: 0.60
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Reproduce the targeted evaluation setup and measure side-task completion and detectability across model
      versions, including with realistic monitoring and human oversight.
    assumptions:
      - The targeted evaluation reflects real-world “suspicious side task” threat models for agents.
    falsifiers:
      - Reproduction fails to show a material increase vs prior models or shows side tasks are readily detectable.

  - id: "RISK-2026-916"
    text: >-
      Anthropic reports it determined Opus 4.6 does not cross either the AI R&D-4 or CBRN-4 capability threshold,
      while noting that confidently ruling out future thresholds is becoming increasingly difficult.
    type: "[F]"
    domain: "RISK"
    evidence_level: "E4"
    credence: 0.65
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Review Anthropic’s RSP threshold definitions and the reported evaluation results (including uplift trials and
      expert red teaming) and compare with external assessments of similar capability thresholds.
    assumptions:
      - Anthropic’s threshold definitions and evaluation proxies meaningfully capture the targeted catastrophic-risk
        capabilities.
    falsifiers:
      - External expert evaluations or reproductions indicate Opus 4.6 meets the AI R&D-4 or CBRN-4 thresholds.

  - id: "RISK-2026-917"
    text: >-
      Anthropic reports that none of 16 internal survey participants believed Opus 4.6 could fully automate the work
      of an entry-level, remote-only Anthropic researcher or engineer given current or near-future elicitation and
      scaffolding.
    type: "[F]"
    domain: "RISK"
    evidence_level: "E4"
    credence: 0.65
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Review the survey instrument, participant selection, and evaluation criteria; repeat the survey with a larger
      and more diverse expert pool and stronger/standardized scaffolding baselines.
    assumptions:
      - The survey participants’ judgments are informed and representative of internal capability understanding.
    falsifiers:
      - Replicated surveys under comparable or stronger scaffolds find many experts judge the threshold is met.

  - id: "RISK-2026-918"
    text: >-
      Anthropic reports that in newly-developed evaluations, Opus 4.5 and 4.6 showed elevated susceptibility to
      harmful misuse in GUI computer-use settings.
    type: "[F]"
    domain: "RISK"
    evidence_level: "E4"
    credence: 0.60
    source_ids: ["anthropic-2026-claude-opus-4-6-system-card"]
    operationalization: >-
      Reproduce the GUI computer-use misuse evaluations with adaptive attackers and clear policy boundaries;
      measure rates of policy-violating assistance with and without external safeguards.
    assumptions:
      - The newly-developed evaluations are representative of realistic misuse attempts in computer-use contexts.
    falsifiers:
      - Reproduction finds low misuse susceptibility under comparable evaluation conditions.

