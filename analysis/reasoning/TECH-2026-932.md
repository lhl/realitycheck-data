# Reasoning: TECH-2026-932

> **Claim**: OpenAI reports GPT-5.3-Codex was trained to avoid data-destructive actions (e.g., rm -rf, git clean -xfd, git reset --hard, push --force) by using RL rollouts with a “user model” that makes conflicting edits and rewarding the model for not reverting user-produced changes; OpenAI also reports improved performance on a destructive-actions avoidance evaluation (0.88 for GPT-5.3-Codex vs 0.76 for GPT-5.2-Codex).
> **Credence**: 0.75 (E4)
> **Domain**: TECH

## Evidence Summary

| Direction | Source | Location | Strength | Summary |
|-----------|--------|----------|----------|---------|
| Supports | [openai-2026-gpt-5-3-codex-system-card](../sources/openai-2026-gpt-5-3-codex-system-card.md) | artifact=pdf; locator=Sec. 4.1.2 + Table 2 (Destructive action avoidance) | 0.8 | System card describes RL mitigation for destructiv... |

## Reasoning Chain

The system card provides a concrete mitigation description (training with a conflicting-edit user model during RL and reinforcing non-reversion of user changes) and reports a sizable improvement on a destructive-actions avoidance evaluation for GPT-5.3-Codex. This supports high credence as a report of OpenAI’s training/measurement, while external replication is needed to confirm real-world reduction in data-loss failures.

## Trail History

| Date | Credence | Evidence | Status | Pass |
|------|----------|----------|--------|------|
| 2026-02-07 | 0.75 | E4 | active | 1 |

## Data (portable)

```yaml
claim_id: "TECH-2026-932"
reasoning_trail_id: "REASON-2026-235"
credence_at_time: 0.75
evidence_level_at_time: "E4"
supporting_evidence: []
contradicting_evidence: []
```
